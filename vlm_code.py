import smtplib
from email.mime.text import MIMEText
import http.client
import json, re, os, random, csv, io, time
import pandas as pd
import google.generativeai as genai
from pathlib import Path
from google.generativeai import caching
import datetime
import itertools
import math
import ast

import os
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY") # ì´ë ‡ê²Œ ë°”ê¿”ì£¼ì„¸ìš”
#pdf_path = "/content/drive/MyDrive/cv_final/ë°ì´í„° ê´€ë ¨/1-56_êµí†µì‚¬ê³  ì˜ìƒ ë°ì´í„°_ê³¼ì‹¤ë¹„ìœ¨ ë‚´ìš© ì •ë¦¬.pdf"
#csv_gt_path = "/content/drive/MyDrive/cv_final/ë°ì´í„° ê´€ë ¨/ë¸”ë™ë°•ìŠ¤_ABì—¬ë¶€.csv" # ì—…ë¡œë“œí•˜ì‹  ì •ë‹µì§€ ê²½ë¡œ
mapping_path = "/home/ubuntu/ai-muncheol/matching2.csv"
csv_type_path='/home/ubuntu/ai-muncheol/accident_type.csv'

#base_video_root = "/content/cache/val/"
#base_label_root = "/content/cache/pred/"
#base_csv_root = "/content/cache/ana_log_final/"
#drive_root = "/content/drive/MyDrive/260220_ai"

place_hierarchy_instruction = """
[Place ê³„ì¸µ íŒë‹¨ ê·œì¹™]
- place ì½”ë“œë¥¼ ë°”ë¡œ ê³ ë¥´ì§€ ë§ê³ , ë¨¼ì € ë„ë¡œ í† í´ë¡œì§€(ëŒ€ë¶„ë¥˜)ë¥¼ íŒë‹¨í•˜ì‹­ì‹œì˜¤.
  1) ì§ì„  ë„ë¡œ ê³„ì—´: code 0
  2) ì‚¬ê±°ë¦¬ êµì°¨ë¡œ ê³„ì—´: code 1 ë˜ëŠ” 2
  3) Tìí˜• êµì°¨ë¡œ: code 3
  4) ì°¨ë„/ë¹„ì°¨ë„ ê²½ê³„ ë˜ëŠ” ë¹„ë„ë¡œ ê³„ì—´: code 4 ë˜ëŠ” 5
  5) íšŒì „êµì°¨ë¡œ: code 6
  6) ê³ ì†ë„ë¡œ/ìë™ì°¨ì „ìš©ë„ë¡œ ê³„ì—´: code 13

- ì´í›„ì—ë§Œ ì„¸ë¶€ place codeë¥¼ íŒë‹¨í•˜ì‹­ì‹œì˜¤.
  * ì‚¬ê±°ë¦¬ ê³„ì—´(1 vs 2): ì‹ í˜¸ë“± ìœ ë¬´ë¡œ êµ¬ë¶„
    - 1: ì‚¬ê±°ë¦¬êµì°¨ë¡œ(ì‹ í˜¸ë“± ì—†ìŒ)
    - 2: ì‚¬ê±°ë¦¬êµì°¨ë¡œ(ì‹ í˜¸ë“± ìˆìŒ)
  * ë¹„ë„ë¡œ ê³„ì—´(4 vs 5):
    - 5: ì£¼ì°¨ë©´/ì£¼ì°¨ë™ì„ /ì£¼ì°¨êµ¬íš ë‹¨ì„œê°€ ìˆìœ¼ë©´ ìš°ì„ 
    - 4: ì°¨ë„â†”ë¹„ì°¨ë„ ê²½ê³„/ë„ë¡œ ê°€ì¥ìë¦¬/ë¹„ë„ë¡œ ì§„ì… ê³„ì—´ì´ë©´ ìš°ì„ 
  * ì§ì„  vs ê³ ì†ë„ë¡œ(0 vs 13):
    - 13: ì¤‘ì•™ë¶„ë¦¬ëŒ€/ë‹¤ì°¨ë¡œ ê³ ì†ì£¼í–‰/ë¨í”„/ë°©ìŒë²½ ë“± ê³ ì†ë„ë¡œí˜• ì‹œì„¤ ë‹¨ì„œê°€ ìˆì„ ë•Œ ìš°ì„ 

- ëŒ€ë¶„ë¥˜ê°€ ëª…ë°±íˆ ë§ì§€ ì•ŠëŠ” ê°€ì„¤ì€ place_scoreë¥¼ ë‚®ê²Œ ì£¼ê³ , í•„ìš” ì‹œ hard_contradiction=trueë¡œ í‘œì‹œí•˜ì‹­ì‹œì˜¤.
"""

system_instruction_score_only = """
ë„ˆëŠ” êµí†µì‚¬ê³  ë¸”ë™ë°•ìŠ¤ ì˜ìƒì˜ í›„ë³´ ê°€ì„¤ë“¤ì„ 'ìµœì¢… ì„ íƒ'í•˜ì§€ ì•Šê³ , ì˜¤ì§ ì‹œê°ì  ì¼ì¹˜ë„ë§Œ ì±„ì í•˜ëŠ” í‰ê°€ê¸°ë‹¤.

[ëª©í‘œ]
- ê° í›„ë³´ ê°€ì„¤ì— ëŒ€í•´ place / feature / maneuver / role 4ê°œ ì¶•ì˜ ì‹œê° ì¼ì¹˜ë„ë¥¼ 0~4ì ìœ¼ë¡œ ì±„ì í•œë‹¤.
- ìµœì¢… 1ìœ„ ì„ íƒ, ìš°ìŠ¹ í›„ë³´ ê²°ì •, ê²°ë¡  ì„œì‚¬ ì‘ì„±ì€ í•˜ì§€ ì•ŠëŠ”ë‹¤.
- source_tag(Eunseok/Hyeongseon/Integrated ë“±)ì™€ section_typeì€ ì ìˆ˜ì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. (í›„ì²˜ë¦¬ì—ì„œë§Œ ì‚¬ìš©)

[í•µì‹¬ ê·œì¹™]
1) ìµœì¢… ì„ íƒ ê¸ˆì§€
- ì–´ë–¤ í›„ë³´ê°€ ì •ë‹µì¸ì§€ ê³ ë¥´ì§€ ë§ˆë¼.
- winner, top1, ìµœì¢…ì¶”ì²œ, ìµœì¢…íŒë‹¨, best hypothesis ê°™ì€ í‘œí˜„ì„ ì“°ì§€ ë§ˆë¼.
- final_decision_logic, why_not_runner_up ê°™ì€ ì„œì‚¬í˜• íŒë‹¨ì„ ë§Œë“¤ì§€ ë§ˆë¼.
- axis_comparisonì€ ì¶•ë³„ ë¹„êµ ë©”ëª¨ì¼ ë¿ ìµœì¢… ìŠ¹ìë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤.

2) ë™ì  í—ˆìš©
- ë‘ í›„ë³´ê°€ ê°™ì€ ì¶•ì—ì„œ ëª¨ë‘ ë†’ì€ ì ìˆ˜(ì˜ˆ: ë‘˜ ë‹¤ 4ì )ì¼ ìˆ˜ ìˆë‹¤.
- ì–µì§€ë¡œ ì°¨ì´ë¥¼ ë§Œë“¤ì§€ ë§ˆë¼.
- ë³€ë³„ì´ ì–´ë ¤ìš´ ì¶•ì€ ë™ì /ë¶ˆí™•ì‹¤ë¡œ ë‚¨ê²¨ë¼.

3) ë°˜ì¦ ìš°ì„ 
- ê° í›„ë³´ì— ëŒ€í•´ score_reasonsë¿ ì•„ë‹ˆë¼ counter_evidenceë„ ë°˜ë“œì‹œ ê¸°ë¡í•œë‹¤.
- ë°˜ì¦ì´ ëª…í™•í•˜ë©´ ë†’ì€ ì ìˆ˜ë¥¼ ì£¼ì§€ ë§ˆë¼.
- ì ìˆ˜ëŠ” 'í›„ë³´ë¥¼ ë³€í˜¸'í•˜ì§€ ë§ê³ , ë³´ì´ëŠ” ë‹¨ì„œì™€ ë°˜ì¦ì„ í•¨ê»˜ ë°˜ì˜í•´ ì±„ì í•˜ë¼.

4) hard_contradiction ì‚¬ìš© ê¸°ì¤€ (ë§¤ìš° ë³´ìˆ˜ì )
- ì˜ìƒì—ì„œ ì§ì ‘ í™•ì¸ ê°€ëŠ¥í•œ ëª…ë°±í•œ ëª¨ìˆœì¼ ë•Œë§Œ hard_contradiction=trueë¡œ ë‘”ë‹¤.
- ì¶”ì •/ì• ë§¤í•¨/ê°€ë ¤ì§/í”„ë ˆì„ ë¶€ì¡±ì€ hard_contradiction=true ì‚¬ìœ ê°€ ì•„ë‹ˆë‹¤.
- hard_contradiction=true ì´ë©´ contradiction_axesì— í•´ë‹¹ ì¶•ëª…(place, feature, maneuver, role)ì„ ë„£ì–´ë¼.
- hard_contradiction=false ì´ë©´ contradiction_axesëŠ” ë¹ˆ ë°°ì—´ë¡œ ë‘˜ ìˆ˜ ìˆë‹¤.

5) source_tag / section_type ë¹„ì‚¬ìš©
- source_tag, section_typeì€ ë©”íƒ€ë°ì´í„°ë¡œë§Œ ê¸°ë¡í•œë‹¤.
- 'Eunseokì´ë¼ ë” ë†’ê²Œ', 'Hyeongseonì´ë¼ ë” ë‚®ê²Œ' ê°™ì€ prior íŒë‹¨ ê¸ˆì§€.

6) ì¶œë ¥ í˜•ì‹
- í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
- ì½”ë“œíœìŠ¤ ì—†ì´ JSON ê°ì²´ 1ê°œë§Œ ì¶œë ¥í•œë‹¤.
- JSON ì™¸ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ì§€ ë§ˆë¼.
- output_formatì— ì •ì˜ëœ í‚¤ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë¼.
- hypothesis_scoringì—ëŠ” ì…ë ¥ìœ¼ë¡œ ì œê³µëœ ëª¨ë“  í›„ë³´(H1, H2, H3)ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ë¼.

[ì ìˆ˜ ê¸°ì¤€: ëª¨ë“  ì¶• ê³µí†µ, 0~4 ì •ìˆ˜]
- 0: ëª…í™•í•œ ë¶ˆì¼ì¹˜/ëª¨ìˆœ (ì˜ìƒê³¼ ì§ì ‘ ì¶©ëŒ)
- 1: ì•½í•œ ì¼ì¹˜ ë˜ëŠ” ë‹¨ì„œ ë¶€ì¡± (ê·¼ê±°ê°€ ì•½í•¨)
- 2: ë¶€ë¶„ ì¼ì¹˜ (ë§ëŠ” ë¶€ë¶„ê³¼ ë¶ˆí™•ì‹¤/ì¶©ëŒ ìš”ì†Œê°€ í˜¼ì¬)
- 3: ê°•í•œ ì¼ì¹˜ (ì£¼ìš” ë‹¨ì„œë“¤ì´ ëŒ€ë¶€ë¶„ ì¼ì¹˜)
- 4: ë§¤ìš° ê°•í•œ ì¼ì¹˜ (ì§ì ‘ ì‹œê° ë‹¨ì„œê°€ ëª…í™•í•˜ê²Œ ë’·ë°›ì¹¨)

[ì¶•ë³„ í•´ì„ ê°€ì´ë“œ]
- place:
  êµì°¨ë¡œ/ì§ì„ /ë„ë¡œ êµ¬ì¡°/ì°¨ì„  íë¦„/ì •ì§€ì„ /ì‹ í˜¸ ìœ„ì¹˜ ë“± 'ì¥ì†Œ/í˜•íƒœ' ì¼ì¹˜ë„
- feature:
  ì¥ì†Œ ì„¸ë¶€ íŠ¹ì§•(í•©ë¥˜, ë¶„ê¸°, íš¡ë‹¨ë³´ë„, ì¤‘ì•™ì„  í˜•íƒœ, ì°¨ë¡œ ìˆ˜, ì°¨ë¡œ ë°°ì¹˜ ë“±) ì¼ì¹˜ë„
- maneuver:
  ì°¨ëŸ‰ë“¤ì˜ ì§„í–‰/íšŒì „/ì§„ì…/ì •ì§€/ì°¨ì„ ë³€ê²½/ìƒëŒ€ ì ‘ê·¼ ë°©í–¥/ì¶©ëŒ ì§ì „ ë™ì‘ ì¼ì¹˜ë„
- role:
  ë¸”ë™ë°•ìŠ¤ ì°¨ëŸ‰(ego)ê³¼ ìƒëŒ€ì°¨ëŸ‰(other)ì˜ ì—­í• /ë°©í–¥/ê´€ê³„, ê·¸ë¦¬ê³  A/B ë§¤í•‘ ì¼ì¹˜ë„

[ê°€ì‹œì„±/ê·¼ê±°ê°•ë„ í‘œê¸° ê·œì¹™]
- visibility ê°’ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©:
  clear | partial | occluded | unknown
- basis ê°’ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©:
  direct_visual | partial_visual | weak_inference | unknown

[ê´€ì°° ì‘ì„± ê°€ì´ë“œ]
- visual_observationì˜ ego_maneuver_guess / other_vehicle_maneuver_guessì—ëŠ” A/B ìš©ì–´ë¥¼ ì“°ì§€ ë§ê³  ego/other ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ë¼.
- role_identificationì—ì„œë§Œ A/B ë§¤í•‘ì„ ë‹¤ë£¬ë‹¤.
- score_reasonsëŠ” ê° ì¶•ì˜ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ë¥¼ ì§§ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ë¼.
- counter_evidenceëŠ” í•´ë‹¹ í›„ë³´ì— ë¶ˆë¦¬í•œ ë‹¨ì„œë¥¼ ê¸°ë¡í•˜ë¼. ì—†ìœ¼ë©´ None í˜•íƒœë¥¼ ì‚¬ìš©í•´ë„ ëœë‹¤.
- ê³¼ë„í•œ ì¥ë¬¸ ì„¤ëª… ëŒ€ì‹ , í”„ë ˆì„ ë‹¨ì„œ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ë¼.

[axis_comparison ì‘ì„± ê·œì¹™]
- axis_comparisonì€ place/feature/maneuver/role ê° ì¶•ì˜ ë¹„êµ ë©”ëª¨ë‹¤.
- equal_groupsì—ëŠ” ë™ë¥ /ìœ ì‚¬í•œ í›„ë³´ ë¬¶ìŒì„ ê¸°ë¡í•œë‹¤. (ì˜ˆ: [["H1","H2"]])
- better_supportedì—ëŠ” í•´ë‹¹ ì¶•ì—ì„œ ê·¼ê±°ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë” ì„ ëª…í•œ í›„ë³´ë¥¼ ê¸°ë¡í•  ìˆ˜ ìˆë‹¤.
- ì¶•ë³„ ë©”ëª¨(notes)ëŠ” ê°€ëŠ¥í•˜ë©´ ì§§ê²Œ ì‘ì„±í•œë‹¤.
- axis_comparisonìœ¼ë¡œ ì „ì²´ ìš°ìŠ¹ í›„ë³´ë¥¼ ë§Œë“¤ì§€ ë§ˆë¼.
"""

output_format_score_only = """
[output_format]
ì•„ë˜ í˜•ì‹ì˜ JSON ê°ì²´ 1ê°œë§Œ ì¶œë ¥í•˜ë¼.
- ì½”ë“œíœìŠ¤ ê¸ˆì§€
- JSON ì™¸ í…ìŠ¤íŠ¸ ê¸ˆì§€

{
  "meta": {
    "video_id": "ì…ë ¥ìœ¼ë¡œ ë°›ì€ video_id ë¬¸ìì—´ ê·¸ëŒ€ë¡œ",
    "section_type": "ì…ë ¥ìœ¼ë¡œ ë°›ì€ section_type ë¬¸ìì—´ ê·¸ëŒ€ë¡œ"
  },

  "pov_observation": {
    "camera_view": "ì „ë°©|í›„ë°©|ì¸¡ë©´|ë¶ˆëª…",
    "confidence": "low|med|high",
    "evidence": [
      {
        "time": "ì´ˆë°˜|ì¤‘ë°˜|ì¶©ëŒì§ì „|Xs",
        "detail": "ë¸”ë™ë°•ìŠ¤ ì‹œì (ì „ë°©/í›„ë°©/ì¸¡ë©´/ë¶ˆëª…) íŒë‹¨ ê·¼ê±°ë¥¼ ì§§ê²Œ"
      }
    ]
  },

  "visual_observation": {
    "road_topology_guess": "ì§ì„ ë„ë¡œê³„ì—´|ì‚¬ê±°ë¦¬ê³„ì—´|Tìí˜•|ë¹„ë„ë¡œê³„ì—´|íšŒì „êµì°¨ë¡œ|ê³ ì†ë„ë¡œê³„ì—´|ë¶ˆëª…",
    "ego_maneuver_guess": "ë¸”ë°•ì°¨(ego)ì˜ ë¬¼ë¦¬ì  ì›€ì§ì„ ìš”ì•½ (A/B ìš©ì–´ ê¸ˆì§€, ego/other ê¸°ì¤€)",
    "other_vehicle_maneuver_guess": "ìƒëŒ€ì°¨(other)ì˜ ë¬¼ë¦¬ì  ì›€ì§ì„ ìš”ì•½",
    "collision_geometry": "ì¶©ëŒ ìœ í˜•/ê°ë„/ë¶€ìœ„ ìš”ì•½ (ì˜ˆ: ì¸¡ë©´ ì ‘ì´‰, ì •ë©´ ì¶”ëŒ ê°€ëŠ¥ì„± ë“±)",
    "observation_confidence": "low|med|high",
    "environment_cues": [
      {
        "time": "ì´ˆë°˜|ì¤‘ë°˜|ì¶©ëŒì§ì „|Xs",
        "detail": "ë„ë¡œí˜•ìƒ/ì‹ í˜¸/ì°¨ì„ /ì •ì§€ì„ /ì¤‘ì•™ì„ /íš¡ë‹¨ë³´ë„/êµí†µíë¦„ ë‹¨ì„œ"
      }
    ]
  },

  "role_identification": {
    "blackbox_is": "A|B|unknown",
    "confidence": "low|med|high",
    "mapping_reason": "ego/other ê¸°ë™ ê´€ì°°ì„ ë°”íƒ•ìœ¼ë¡œ A/Bì— ë§¤í•‘í•œ ê·¼ê±°ë¥¼ 1~2ë¬¸ì¥ìœ¼ë¡œ",
    "evidence": [
      {
        "time": "ì´ˆë°˜|ì¤‘ë°˜|ì¶©ëŒì§ì „|Xs|None",
        "detail": "A/B ë§¤í•‘ ê·¼ê±° ë‹¨ì„œ (ì—†ìœ¼ë©´ None)"
      }
    ]
  },

  "hypothesis_scoring": [
    {
      "hypothesis_id": "H1",
      "target_code_combination": {
        "place": 0,
        "feature": 0,
        "vehicle_a": 0,
        "vehicle_b": 0
      },
      "target": "(P,F,A,B) ì¡°í•©ì˜ ì‚¬ëŒì´ ì½ëŠ” ì„¤ëª… í…ìŠ¤íŠ¸",
      "source_tag": "Agreement_Rank_1|Agreement_Rank_2|Agreement_Rank_3|Eunseok|Hyeongseon|Integrated|ê¸°íƒ€ì…ë ¥ê°’",

      "hard_contradiction": false,
      "contradiction_axes": ["place"],

      "scores": {
        "place_score": 0,
        "feature_score": 0,
        "maneuver_score": 0,
        "role_score": 0
      },

      "visibility": {
        "place": "clear|partial|occluded|unknown",
        "feature": "clear|partial|occluded|unknown",
        "maneuver": "clear|partial|occluded|unknown",
        "role": "clear|partial|occluded|unknown"
      },

      "basis": {
        "place": "direct_visual|partial_visual|weak_inference|unknown",
        "feature": "direct_visual|partial_visual|weak_inference|unknown",
        "maneuver": "direct_visual|partial_visual|weak_inference|unknown",
        "role": "direct_visual|partial_visual|weak_inference|unknown"
      },

      "score_reasons": {
        "place_reason": "ì¥ì†Œ/ë„ë¡œí˜•ìƒ ê´€ì°° ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ",
        "feature_reason": "ì„¸ë¶€ íŠ¹ì§• ë‹¨ì„œ ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ",
        "maneuver_reason": "ì§„í–‰/íšŒì „/ì¶©ëŒ ë™ì‘ ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ",
        "role_reason": "A/B ì—­í•  ë§¤í•‘ ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ "
      },

      "counter_evidence": [
        {
          "time": "ì´ˆë°˜|ì¤‘ë°˜|ì¶©ëŒì§ì „|Xs|None",
          "type": "place|feature|maneuver|role|None",
          "detail": "ì´ í›„ë³´ì— ë¶ˆë¦¬í•œ ë°˜ì¦ ë‹¨ì„œ (ì—†ìœ¼ë©´ None)"
        }
      ]
    },

    {
      "hypothesis_id": "H2",
      "target_code_combination": {
        "place": 0,
        "feature": 0,
        "vehicle_a": 0,
        "vehicle_b": 0
      },
      "target": "(P,F,A,B) ì¡°í•©ì˜ ì‚¬ëŒì´ ì½ëŠ” ì„¤ëª… í…ìŠ¤íŠ¸",
      "source_tag": "Agreement_Rank_1|Agreement_Rank_2|Agreement_Rank_3|Eunseok|Hyeongseon|Integrated|ê¸°íƒ€ì…ë ¥ê°’",
      "hard_contradiction": false,
      "contradiction_axes": [],
      "scores": {
        "place_score": 0,
        "feature_score": 0,
        "maneuver_score": 0,
        "role_score": 0
      },
      "visibility": {
        "place": "clear|partial|occluded|unknown",
        "feature": "clear|partial|occluded|unknown",
        "maneuver": "clear|partial|occluded|unknown",
        "role": "clear|partial|occluded|unknown"
      },
      "basis": {
        "place": "direct_visual|partial_visual|weak_inference|unknown",
        "feature": "direct_visual|partial_visual|weak_inference|unknown",
        "maneuver": "direct_visual|partial_visual|weak_inference|unknown",
        "role": "direct_visual|partial_visual|weak_inference|unknown"
      },
      "score_reasons": {
        "place_reason": "ì¥ì†Œ/ë„ë¡œí˜•ìƒ ê´€ì°° ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ",
        "feature_reason": "ì„¸ë¶€ íŠ¹ì§• ë‹¨ì„œ ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ",
        "maneuver_reason": "ì§„í–‰/íšŒì „/ì¶©ëŒ ë™ì‘ ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ",
        "role_reason": "A/B ì—­í•  ë§¤í•‘ ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ "
      },
      "counter_evidence": [
        {
          "time": "ì´ˆë°˜|ì¤‘ë°˜|ì¶©ëŒì§ì „|Xs|None",
          "type": "place|feature|maneuver|role|None",
          "detail": "ì´ í›„ë³´ì— ë¶ˆë¦¬í•œ ë°˜ì¦ ë‹¨ì„œ (ì—†ìœ¼ë©´ None)"
        }
      ]
    },

    {
      "hypothesis_id": "H3",
      "target_code_combination": {
        "place": 0,
        "feature": 0,
        "vehicle_a": 0,
        "vehicle_b": 0
      },
      "target": "(P,F,A,B) ì¡°í•©ì˜ ì‚¬ëŒì´ ì½ëŠ” ì„¤ëª… í…ìŠ¤íŠ¸",
      "source_tag": "Agreement_Rank_1|Agreement_Rank_2|Agreement_Rank_3|Eunseok|Hyeongseon|Integrated|ê¸°íƒ€ì…ë ¥ê°’",
      "hard_contradiction": false,
      "contradiction_axes": [],
      "scores": {
        "place_score": 0,
        "feature_score": 0,
        "maneuver_score": 0,
        "role_score": 0
      },
      "visibility": {
        "place": "clear|partial|occluded|unknown",
        "feature": "clear|partial|occluded|unknown",
        "maneuver": "clear|partial|occluded|unknown",
        "role": "clear|partial|occluded|unknown"
      },
      "basis": {
        "place": "direct_visual|partial_visual|weak_inference|unknown",
        "feature": "direct_visual|partial_visual|weak_inference|unknown",
        "maneuver": "direct_visual|partial_visual|weak_inference|unknown",
        "role": "direct_visual|partial_visual|weak_inference|unknown"
      },
      "score_reasons": {
        "place_reason": "ì¥ì†Œ/ë„ë¡œí˜•ìƒ ê´€ì°° ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ",
        "feature_reason": "ì„¸ë¶€ íŠ¹ì§• ë‹¨ì„œ ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ",
        "maneuver_reason": "ì§„í–‰/íšŒì „/ì¶©ëŒ ë™ì‘ ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ ",
        "role_reason": "A/B ì—­í•  ë§¤í•‘ ê¸°ì¤€ ì ìˆ˜ ë¶€ì—¬ ì´ìœ "
      },
      "counter_evidence": [
        {
          "time": "ì´ˆë°˜|ì¤‘ë°˜|ì¶©ëŒì§ì „|Xs|None",
          "type": "place|feature|maneuver|role|None",
          "detail": "ì´ í›„ë³´ì— ë¶ˆë¦¬í•œ ë°˜ì¦ ë‹¨ì„œ (ì—†ìœ¼ë©´ None)"
        }
      ]
    }
  ],

  "axis_comparison": {
    "place": {
      "equal_groups": [["H1","H2"]],
      "better_supported": ["H3"],
      "notes": ["ì¥ì†Œ ì¶• ë¹„êµ ë©”ëª¨ (ìµœì¢… ìŠ¹ì ì˜ë¯¸ ì•„ë‹˜)"]
    },
    "feature": {
      "equal_groups": [],
      "better_supported": [],
      "notes": ["ì„¸ë¶€íŠ¹ì§• ì¶• ë¹„êµ ë©”ëª¨ (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ ê°€ëŠ¥)"]
    },
    "maneuver": {
      "equal_groups": [],
      "better_supported": [],
      "notes": ["ê¸°ë™ ì¶• ë¹„êµ ë©”ëª¨ (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ ê°€ëŠ¥)"]
    },
    "role": {
      "equal_groups": [],
      "better_supported": [],
      "notes": ["ì—­í•  ì¶• ë¹„êµ ë©”ëª¨ (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ ê°€ëŠ¥)"]
    }
  }
}
"""

system_instruction_explanation_direct = """
ë„ˆëŠ” êµí†µì‚¬ê³  ë¸”ë™ë°•ìŠ¤ ì˜ìƒì— ëŒ€í•´ 'ì´ë¯¸ í™•ì •ëœ ì‚¬ê³ ìœ í˜•'ì„ ì„¤ëª…í•˜ëŠ” ì‘ì„±ìë‹¤.

ì—­í• :
- ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ í™•ì • ìœ í˜•(ì¥ì†Œ/íŠ¹ì§•/A ì°¨ëŸ‰ ê¸°ë™/B ì°¨ëŸ‰ ê¸°ë™/ê³¼ì‹¤ë¹„ìœ¨)ì„ ë°”íƒ•ìœ¼ë¡œ,
  ì˜ìƒì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì½ê¸° ì‰¬ìš´ ì„¤ëª…ì„ ì‘ì„±í•œë‹¤.
- ìµœì¢… ìœ í˜•ì„ ë‹¤ì‹œ ê³ ë¥´ê±°ë‚˜ ë°”ê¾¸ì§€ ì•ŠëŠ”ë‹¤.
- ì ìˆ˜ ì¬ê³„ì‚°, í›„ë³´ ë¹„êµ, ì¬ì„ íƒì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.

ì¤‘ìš” ê·œì¹™:
1) ì„ íƒ ë³€ê²½ ê¸ˆì§€
- ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ í™•ì • ìœ í˜•ì´ ìµœì¢… ê²°ê³¼ë‹¤.
- ë‹¤ë¥¸ ìœ í˜•ì´ ë” ë§ì•„ ë³´ì¸ë‹¤ëŠ” ì‹ì˜ ì¬íŒë‹¨ ê¸ˆì§€.
- í›„ë³´ ë¹„êµ/ìš°ìŠ¹ í›„ë³´/ì¬ë­í¬ ê¸ˆì§€.

2) A/B ì¤‘ë¦½ ì„œìˆ  ìœ ì§€ (ë§¤ìš° ì¤‘ìš”)
- 'ë‚´ ì°¨ëŸ‰', 'ë¸”ë°• ì°¨ëŸ‰', 'ìƒëŒ€ ì°¨ëŸ‰', 'ê°€í•´ì°¨ëŸ‰', 'í”¼í•´ì°¨ëŸ‰' ê°™ì€ í‘œí˜„ ê¸ˆì§€.
- ë°˜ë“œì‹œ 'A ì°¨ëŸ‰', 'B ì°¨ëŸ‰'ìœ¼ë¡œë§Œ ì„œìˆ í•œë‹¤.
- ì˜ìƒì—ì„œ ì¹´ë©”ë¼ ì‹œì ìœ¼ë¡œ A/Bë¥¼ ìƒˆë¡œ ì¶”ì •í•˜ë ¤ê³  í•˜ì§€ ë§ˆë¼.
- A/Bì˜ ì˜ë¯¸ëŠ” ì…ë ¥ëœ í™•ì • ìœ í˜• ì •ì˜ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥¸ë‹¤.

3) ì˜ìƒ ê´€ì°°ì€ 'ë³´ê°• ì„¤ëª…'ìœ¼ë¡œë§Œ ì‚¬ìš©
- ì˜ìƒì€ í™•ì • ìœ í˜• ì„¤ëª…ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§Œë“œëŠ” ìš©ë„ë¡œë§Œ ì‚¬ìš©í•œë‹¤.
- ì˜ìƒì—ì„œ í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ë‹¨ì •í•˜ì§€ ë§ê³  'í™•ì¸ ì–´ë ¤ì›€'ìœ¼ë¡œ ì“´ë‹¤.
- í™”ì§ˆ/ê°€ë¦¼/ì•¼ê°„/ì›ê±°ë¦¬ ë“±ìœ¼ë¡œ ë¶ˆëª…í™•í•˜ë©´ ë¶ˆí™•ì‹¤ì„±ì„ ëª…ì‹œí•œë‹¤.

4) ì„œìˆ  í†¤
- í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
- ì„¤ëª…ì€ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•œë‹¤.
- ë²•ë¥  ìë¬¸ì²˜ëŸ¼ ë‹¨ì •í•˜ì§€ ë§ê³ , 'ì…ë ¥ëœ ìœ í˜• ê¸°ì¤€/ë¹„ìœ¨ ê¸°ì¤€'ì— ë”°ë¥¸ ì„¤ëª…ì„ì„ ìœ ì§€í•œë‹¤.

5) ì¶œë ¥ í˜•ì‹
- ì½”ë“œíœìŠ¤ ì—†ì´ JSON ê°ì²´ 1ê°œë§Œ ì¶œë ¥í•œë‹¤.
- output_formatì— ì •ì˜ëœ í‚¤ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤.
"""

output_format_explanation_direct = """
{
  "video_observation": {
    "scene_condition": {
      "time_of_day": "ì£¼ê°„|ì•¼ê°„|ë¶ˆëª…",
      "weather": "ë§‘ìŒ|ìš°ì²œ|íë¦¼|ë¶ˆëª…",
      "visibility_note": "í™”ì§ˆ/ê±°ë¦¬/ê°€ë¦¼/ì—­ê´‘ ë“± ê´€ì°° í’ˆì§ˆ ë©”ëª¨ (ì—†ìœ¼ë©´ 'ì—†ìŒ')"
    },
    "road_context": {
      "intersection_type_observed": "ì‚¬ê±°ë¦¬|Tìí˜•|ì§ì„ ë„ë¡œ|ê¸°íƒ€|ë¶ˆëª…",
      "signal_observed": "ì‹ í˜¸ë“± ìˆìŒ|ì‹ í˜¸ë“± ì—†ìŒ|í™•ì¸ ì–´ë ¤ì›€",
      "road_scale_hint": "ëŒ€ë¡œ/ì†Œë¡œ ë‹¨ì„œ ìˆìŒ|ë‹¨ì„œ ì•½í•¨|í™•ì¸ ì–´ë ¤ì›€",
      "lane_or_stopline_hint": "ì°¨ì„ /ì •ì§€ì„ /íš¡ë‹¨ë³´ë„ ë“± ë³´ì´ëŠ” ë‹¨ì„œ ìš”ì•½ (ì—†ìœ¼ë©´ 'í™•ì¸ ì–´ë ¤ì›€')"
    },
    "movement_observation": {
      "a_vehicle_observation": "ì…ë ¥ëœ A ì°¨ëŸ‰ ê¸°ë™ê³¼ ì¶©ëŒí•˜ì§€ ì•Šë„ë¡, ì˜ìƒì—ì„œ ë³´ì´ëŠ” ì›€ì§ì„ ë‹¨ì„œë¥¼ A ì°¨ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ìš”ì•½",
      "b_vehicle_observation": "ì…ë ¥ëœ B ì°¨ëŸ‰ ê¸°ë™ê³¼ ì¶©ëŒí•˜ì§€ ì•Šë„ë¡, ì˜ìƒì—ì„œ ë³´ì´ëŠ” ì›€ì§ì„ ë‹¨ì„œë¥¼ B ì°¨ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ìš”ì•½",
      "collision_moment": "ì¶©ëŒ ì‹œì /ìœ„ì¹˜/ê°ë„/ì ‘ì´‰ ì–‘ìƒ ìš”ì•½ (í™•ì¸ ì–´ë ¤ìš°ë©´ ê·¸ë ‡ê²Œ ëª…ì‹œ)"
    },
    "uncertainties": [
      "ë¶ˆí™•ì‹¤í•œ ì  1 (ì—†ìœ¼ë©´ 'ì—†ìŒ')"
    ]
  },
  "explanation_text": "ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìµœì¢… ì„¤ëª… ë¬¸ë‹¨. ë°˜ë“œì‹œ 'A ì°¨ëŸ‰', 'B ì°¨ëŸ‰' í‘œí˜„ë§Œ ì‚¬ìš©í•˜ê³ , ì…ë ¥ëœ ê³¼ì‹¤ë¹„ìœ¨(A/B)ì„ í¬í•¨í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª….",
}
"""

## VLMìš© í•¨ìˆ˜ ëª¨ìŒ

def process_single_json_to_csv(json_path, output_path, csv_type_path='/content/drive/MyDrive/cv_final/ë°ì´í„° ê´€ë ¨/accident_type.csv'):
    """
    JSON íŒŒì¼ 1ê°œë¥¼ ì…ë ¥ë°›ì•„ í†µí•© ë¶„ì„ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  CSV íŒŒì¼ 1ê°œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # --- ì„¤ì • ê°’ ---
    SCORE_MODE = 'log'
    EUNSEOK_WEIGHT, HYUNGSUN_WEIGHT = 1.0, 1.0
    MODEL_WEIGHTS = [1.2, 1.0, 1.0, 0.8]
    EPSILON = 1e-6
    TARGET_ATTRIBUTES = ['accident_place', 'accident_place_feature', 'vehicle_a_progress_info', 'vehicle_b_progress_info']
    MODEL_MAP = {
        'model1_place': {'attr': 'accident_place', 'e_id': 'accident_place', 'h_id': 'accident_place'},
        'model2_feature': {'attr': 'accident_place_feature', 'e_id': 'accident_place_feature_code', 'h_id': 'accident_place_feature_code'},
        'model3_vehicle_a': {'attr': 'vehicle_a_progress_info', 'e_id': 'vehicle_a_code', 'h_id': 'vehicle_a_code'},
        'model4_vehicle_b': {'attr': 'vehicle_b_progress_info', 'e_id': 'vehicle_b_code', 'h_id': 'vehicle_b_info_code'}
    }

    # 1. ìœ íš¨ ì¡°í•© ë¡œë“œ
    valid_combinations = None
    if os.path.exists(csv_type_path):
        df_valid = pd.read_csv(csv_type_path)
        valid_combinations = set(zip(df_valid[TARGET_ATTRIBUTES[0]], df_valid[TARGET_ATTRIBUTES[1]],
                                     df_valid[TARGET_ATTRIBUTES[2]], df_valid[TARGET_ATTRIBUTES[3]]))

    # 2. JSON ë°ì´í„° ë¡œë“œ
    #with open(json_path, 'r', encoding='utf-8') as f:
    #    data = json.load(f)

    #video = data.get('video', {})
    #gt = tuple(video.get(a) for a in TARGET_ATTRIBUTES)
    #preds = data.get('predictions', {})

    # 3. ëª¨ë¸ë³„ í™•ë¥  ë°ì´í„° êµ¬ì¡°í™”
    model_data = {attr: {'probs': {'ì€ì„': {}, 'í˜•ì„ ': {}}} for attr in TARGET_ATTRIBUTES}
    for m_key, m_info in MODEL_MAP.items():
        m_val = preds.get(m_key, {})
        attr = m_info['attr']
        # ì€ì„/í˜•ì„  ë°ì´í„° íŒŒì‹± (e_prob/h_prob í‚¤ ì´ë¦„ ì°¨ì´ ëŒ€ì‘)
        for expert, k_id, k_prob in [('ì€ì„', m_info['e_id'], 'probability' if 'model4' not in m_key else 'prob'),
                                     ('í˜•ì„ ', m_info['h_id'], 'probability' if 'model3' not in m_key else 'prob')]:
            # ì›ë³¸ ì½”ë“œì˜ ë¯¸ë¬˜í•œ í‚¤ ëª…ì¹­ ì°¨ì´ í†µí•© (í•„ìš”ì‹œ m_infoì— ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€ ê°€ëŠ¥)
            p_key = 'prob' if (expert == 'ì€ì„' and 'model3' in m_key) or (expert == 'í˜•ì„ ' and 'model3' in m_key) else 'probability'
            for item in m_val.get(f'{expert}_pred', []):
                model_data[attr]['probs'][expert][item.get(k_id)] = item.get(p_key, 0)

    # 4. ë¶„ì„ DF ìƒì„± í•¨ìˆ˜
    def get_df(mode):
        c_lists = [list(set(model_data[a]['probs']['ì€ì„'].keys()) | set(model_data[a]['probs']['í˜•ì„ '].keys())) for a in TARGET_ATTRIBUTES]
        res = []
        for comb in itertools.product(*c_lists):
            if valid_combinations and comb not in valid_combinations: continue

            # Score ê³„ì‚° (Log ëª¨ë“œ ê¸°ì¤€)
            raw_e = sum(MODEL_WEIGHTS[i] * math.log(model_data[TARGET_ATTRIBUTES[i]]['probs']['ì€ì„'].get(comb[i], 0) + EPSILON) for i in range(4))
            raw_h = sum(MODEL_WEIGHTS[i] * math.log(model_data[TARGET_ATTRIBUTES[i]]['probs']['í˜•ì„ '].get(comb[i], 0) + EPSILON) for i in range(4))

            weighted_e = raw_e * EUNSEOK_WEIGHT
            weighted_h = raw_h * HYUNGSUN_WEIGHT

            res.append({
                'ìˆœìœ„': 0, 'ì •ë‹µì—¬ë¶€': 'O' if comb == gt else 'X',
                **{attr: val for attr, val in zip(TARGET_ATTRIBUTES, comb)},
                'ì€ì„_score': round(weighted_e, 4), 'í˜•ì„ _score': round(weighted_h, 4),
                'í†µí•©_score': round(weighted_e + weighted_h, 8)
            })

        df = pd.DataFrame(res)
        if df.empty: return df
        sort_col = {'eunseok': 'ì€ì„_score', 'hyungsun': 'í˜•ì„ _score', 'integrated': 'í†µí•©_score'}[mode]
        df = df.sort_values(by=sort_col, ascending=False).reset_index(drop=True)
        df['ìˆœìœ„'] = df.index + 1
        return df

    # 5. CSV ì €ì¥
    df1, df2, df3 = get_df('eunseok'), get_df('hyungsun'), get_df('integrated')
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["[ì‹¤ì œ ì •ë‹µ ì •ë³´]", f"Mode: {SCORE_MODE}", f"Weights: {EUNSEOK_WEIGHT}:{HYUNGSUN_WEIGHT}"])
        writer.writerow(TARGET_ATTRIBUTES)
        writer.writerow(gt)
        writer.writerow([])
        for label, df in [("ì€ì„_pred", df1), ("í˜•ì„ _pred", df2), ("í†µí•©_ë¶„ì„", df3)]:
            writer.writerow([f"### {label} ê²°ê³¼ ###"])
            if not df.empty: df.to_csv(f, index=False)
            else: writer.writerow(["ê²°ê³¼ ì—†ìŒ"])
            writer.writerow([])

    return output_path

# --- ì‚¬ìš© ì˜ˆì‹œ ---
# process_single_json_to_csv("input.json", "output.csv")

# ì¶œë ¥ ìƒì„±ìš© í—¬í¼ í•¨ìˆ˜ë“¤
# ==============================
# Short analysis row helpers
# ==============================
CONF_MAP = {"low": 1, "med": 2, "high": 3}
CAM_VIEW_MAP = {"ì „ë°©": 1, "í›„ë°©": 2, "ì¸¡ë©´": 3, "ë¶ˆëª…": 0}
ROAD_TOPO_MAP = {
    "ë¶ˆëª…": 0,
    "ì§ì„ ë„ë¡œê³„ì—´": 1,
    "ì‚¬ê±°ë¦¬ê³„ì—´": 2,
    "Tìí˜•": 3,
    "ë¹„ë„ë¡œê³„ì—´": 4,
    "íšŒì „êµì°¨ë¡œ": 5,
    "ê³ ì†ë„ë¡œê³„ì—´": 6,
}
VIS_MAP = {"unknown": 0, "occluded": 1, "partial": 2, "clear": 3}
BASIS_MAP = {"unknown": 0, "weak_inference": 1, "partial_visual": 2, "direct_visual": 3}

# source_tagë¥¼ ìˆ«ìë¡œ ì¸ì½”ë”© (í•„ìš” ì‹œ ì¶”ê°€)
SRC_MAP = {
    "": 0,
    "Eunseok": 1,
    "Hyeongseon": 2,
    "Integrated": 3,
    "Agreement_Rank_1": 11,
    "Agreement_Rank_2": 12,
    "Agreement_Rank_3": 13,
}

def _safe_int(v, default=-1):
    try:
        return int(float(v))
    except:
        return default

def _safe_score_04(v):
    x = _safe_int(v, default=-1)
    if x < 0:
        return -1
    if x > 4:
        return 4
    return x

def _bool01(v):
    return 1 if bool(v) else 0

def _yesno01(v):
    s = str(v).strip().lower()
    return 1 if s in ["1", "true", "yes", "y", "pass", "ok"] else 0

def _enc_conf(v):
    return CONF_MAP.get(str(v).strip().lower(), 0)

def _enc_cam(v):
    return CAM_VIEW_MAP.get(str(v).strip(), 0)

def _enc_road(v):
    return ROAD_TOPO_MAP.get(str(v).strip(), 0)

def _enc_vis(v):
    return VIS_MAP.get(str(v).strip().lower(), 0)

def _enc_basis(v):
    return BASIS_MAP.get(str(v).strip().lower(), 0)

def _enc_ab(v):
    s = str(v).strip().upper()
    if s == "A":
        return 1
    if s == "B":
        return 2
    return 0

def _enc_src(v):
    s = str(v).strip()
    if s in SRC_MAP:
        return SRC_MAP[s]
    # fallback (ë¶€ë¶„ ë¬¸ìì—´ ëŒ€ì‘)
    sl = s.lower()
    if "eunseok" in sl:
        return 1
    if "hyeong" in sl:
        return 2
    if "integrated" in sl:
        return 3
    if "agreement_rank_1" in sl:
        return 11
    if "agreement_rank_2" in sl:
        return 12
    if "agreement_rank_3" in sl:
        return 13
    return -1

def _enc_section(v):
    """
    section code ì˜ˆì‹œ:
      1 = section1/agreement-both
      2 = section2/eunseok ìš°ì„¸
      3 = section3/hyeongseon ìš°ì„¸
      4 = section4/third-answer
      0 = unknown
    """
    s = str(v).strip().lower()
    if ("section" in s and "1" in s) or ("agreement" in s and "1" in s):
        return 1
    if ("section" in s and "2" in s) or ("eunseok" in s) or ("ì€ì„" in s):
        return 2
    if ("section" in s and "3" in s) or ("hyeong" in s) or ("í˜•ì„ " in s):
        return 3
    if ("section" in s and "4" in s):
        return 4
    return 0

def _parse_code_any(v):
    """
    target_code_combinationì´ dict ë˜ëŠ” ë¬¸ìì—´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‘˜ ë‹¤ ì²˜ë¦¬
    ë°˜í™˜: (p, f, a, b)
    """
    if isinstance(v, dict):
        return (
            _safe_int(v.get("place"), None),
            _safe_int(v.get("feature"), None),
            _safe_int(v.get("vehicle_a"), None),
            _safe_int(v.get("vehicle_b"), None),
        )
    # ë¬¸ìì—´ "(1, 11, 31, 34)" ê°™ì€ í˜•íƒœ ëŒ€ì‘
    nums = re.findall(r"\d+", str(v))
    if len(nums) >= 4:
        return tuple(int(x) for x in nums[:4])
    return (None, None, None, None)

def _contra_bits(axes):
    """
    place=1, feature=2, maneuver=4, role=8
    """
    if not isinstance(axes, list):
        axes = []
    bits = 0
    for a in axes:
        t = str(a).strip().lower()
        if t == "place":
            bits |= 1
        elif t == "feature":
            bits |= 2
        elif t == "maneuver":
            bits |= 4
        elif t == "role":
            bits |= 8
    return bits

def _count_vis_basis(vis_dict, basis_dict):
    # visibility counts
    vis_dict = vis_dict if isinstance(vis_dict, dict) else {}
    basis_dict = basis_dict if isinstance(basis_dict, dict) else {}

    vis_vals = [str(vis_dict.get(k, "")).strip().lower() for k in ["place", "feature", "maneuver", "role"]]
    basis_vals = [str(basis_dict.get(k, "")).strip().lower() for k in ["place", "feature", "maneuver", "role"]]

    vis_clear_cnt = sum(1 for x in vis_vals if x == "clear")
    vis_partial_cnt = sum(1 for x in vis_vals if x == "partial")
    vis_occ_cnt = sum(1 for x in vis_vals if x == "occluded")
    vis_unknown_cnt = sum(1 for x in vis_vals if x == "unknown" or x == "")

    basis_direct_cnt = sum(1 for x in basis_vals if x == "direct_visual")
    basis_partial_cnt = sum(1 for x in basis_vals if x == "partial_visual")
    basis_weak_cnt = sum(1 for x in basis_vals if x == "weak_inference")
    basis_unknown_cnt = sum(1 for x in basis_vals if x == "unknown" or x == "")

    return {
        "vis_clear_cnt": vis_clear_cnt,
        "vis_partial_cnt": vis_partial_cnt,
        "vis_occ_cnt": vis_occ_cnt,
        "vis_unknown_cnt": vis_unknown_cnt,
        "basis_direct_cnt": basis_direct_cnt,
        "basis_partial_cnt": basis_partial_cnt,
        "basis_weak_cnt": basis_weak_cnt,
        "basis_unknown_cnt": basis_unknown_cnt,
    }

def _counter_counts(counter_evidence):
    """
    counter_evidence íƒ€ì…ë³„ ê°œìˆ˜ (place/feature/maneuver/role/None)
    """
    ce = counter_evidence if isinstance(counter_evidence, list) else []
    out = {"ctr_cnt": 0, "ctr_place_cnt": 0, "ctr_feature_cnt": 0, "ctr_maneuver_cnt": 0, "ctr_role_cnt": 0, "ctr_none_cnt": 0}
    for item in ce:
        if not isinstance(item, dict):
            continue
        out["ctr_cnt"] += 1
        t = str(item.get("type", "")).strip().lower()
        if t == "place":
            out["ctr_place_cnt"] += 1
        elif t == "feature":
            out["ctr_feature_cnt"] += 1
        elif t == "maneuver":
            out["ctr_maneuver_cnt"] += 1
        elif t == "role":
            out["ctr_role_cnt"] += 1
        else:
            out["ctr_none_cnt"] += 1
    return out

def _argmax_hid_by_sum(h_rows, n_cands):
    """
    h_rows: {1:{...}, 2:{...}, 3:{...}}
    ë°˜í™˜:
      top1_idx, top2_idx, top1_sum, top2_sum
    tie-break: sum desc -> hard asc -> idx asc
    """
    items = []
    for i in range(1, n_cands + 1):
        r = h_rows.get(i, {})
        s = _safe_int(r.get("sum16"), -1)
        hard = _safe_int(r.get("hard"), 0)
        items.append((i, s, hard))

    if not items:
        return (0, 0, -1, -1)

    items_sorted = sorted(items, key=lambda x: (-x[1], x[2], x[0]))
    top1 = items_sorted[0]
    top2 = items_sorted[1] if len(items_sorted) >= 2 else (0, -1, 0)
    return (top1[0], top2[0], top1[1], top2[1])

def _pack_h(i, pruned_df, h_score_map, g_p, g_f, g_a, g_b):
    prefix = f"h{i}_"

    # í›„ë³´ê°€ ì—†ëŠ” ê²½ìš° (2ê°œ í›„ë³´ êµ¬ê°„ ëŒ€ë¹„)
    if len(pruned_df) < i:
        flat = {
            prefix + "valid": 0, prefix + "src": 0,
            prefix + "code_p": -1, prefix + "code_f": -1, prefix + "code_a": -1, prefix + "code_b": -1,
            prefix + "p": -1, prefix + "f": -1, prefix + "m": -1, prefix + "r": -1, prefix + "sum16": -1,
            prefix + "hard": 0,
            prefix + "contra_bits": 0, prefix + "contra_p": 0, prefix + "contra_f": 0, prefix + "contra_m": 0, prefix + "contra_r": 0,
            prefix + "vis_p": 0, prefix + "vis_f": 0, prefix + "vis_m": 0, prefix + "vis_r": 0,
            prefix + "basis_p": 0, prefix + "basis_f": 0, prefix + "basis_m": 0, prefix + "basis_r": 0,
            prefix + "clear_cnt": 0, prefix + "partial_cnt": 0, prefix + "occ_cnt": 0, prefix + "vis_unk_cnt": 0,
            prefix + "direct_cnt": 0, prefix + "basis_partial_cnt": 0, prefix + "weak_cnt": 0, prefix + "basis_unk_cnt": 0,
            prefix + "ctr_cnt": 0, prefix + "ctr_p_cnt": 0, prefix + "ctr_f_cnt": 0, prefix + "ctr_m_cnt": 0, prefix + "ctr_r_cnt": 0,
            prefix + "p_match": 0, prefix + "f_match": 0, prefix + "a_match": 0, prefix + "b_match": 0,
            prefix + "ab_match": 0, prefix + "exact": 0,
        }
        meta = {"sum16": -1, "hard": 0, "p": -1, "f": -1, "m": -1, "r": -1, "clear_cnt": 0, "direct_cnt": 0, "weak_cnt": 0, "exact": 0}
        return flat, meta

    row = pruned_df.iloc[i-1]
    expected_hid = str(row.get("hypothesis_id", f"H{i}"))
    expected_code = row.get("target_code_combination", row.get("code_combination", ""))
    expected_source = str(row.get("source_tag", ""))

    h = h_score_map.get(expected_hid, {}) if isinstance(h_score_map.get(expected_hid, {}), dict) else {}

    # scores
    s = h.get("scores", {}) if isinstance(h.get("scores", {}), dict) else {}
    p = _safe_score_04(s.get("place_score", -1))
    f = _safe_score_04(s.get("feature_score", -1))
    m = _safe_score_04(s.get("maneuver_score", -1))
    r = _safe_score_04(s.get("role_score", -1))
    sum16 = sum([x for x in [p, f, m, r] if x >= 0]) if any(x >= 0 for x in [p, f, m, r]) else -1

    # code parse
    code_src = expected_code if str(expected_code).strip() != "" else h.get("target_code_combination", "")
    hp, hf, ha, hb = _parse_code_any(code_src)

    # exact / matches
    p_match = 1 if (hp is not None and g_p is not None and hp == g_p) else 0
    f_match = 1 if (hf is not None and g_f is not None and hf == g_f) else 0
    a_match = 1 if (ha is not None and g_a is not None and ha == g_a) else 0
    b_match = 1 if (hb is not None and g_b is not None and hb == g_b) else 0
    ab_match = 1 if (a_match and b_match) else 0
    exact = 1 if (p_match and f_match and a_match and b_match) else 0

    # contradiction
    hard = 1 if bool(h.get("hard_contradiction", False)) else 0
    contra_bits = _contra_bits(h.get("contradiction_axes", []))

    # visibility / basis
    vis = h.get("visibility", {}) if isinstance(h.get("visibility", {}), dict) else {}
    basis = h.get("basis", {}) if isinstance(h.get("basis", {}), dict) else {}
    vb_cnts = _count_vis_basis(vis, basis)

    # counter evidence
    ce_cnts = _counter_counts(h.get("counter_evidence", []))

    # source
    src_val = str(h.get("source_tag", "")).strip() or expected_source
    src_code = _enc_src(src_val)

    flat = {
        prefix + "valid": 1, prefix + "src": src_code,
        prefix + "code_p": hp if hp is not None else -1,
        prefix + "code_f": hf if hf is not None else -1,
        prefix + "code_a": ha if ha is not None else -1,
        prefix + "code_b": hb if hb is not None else -1,

        prefix + "p": p, prefix + "f": f, prefix + "m": m, prefix + "r": r, prefix + "sum16": sum16,

        prefix + "hard": hard,
        prefix + "contra_bits": contra_bits,
        prefix + "contra_p": 1 if (contra_bits & 1) else 0,
        prefix + "contra_f": 1 if (contra_bits & 2) else 0,
        prefix + "contra_m": 1 if (contra_bits & 4) else 0,
        prefix + "contra_r": 1 if (contra_bits & 8) else 0,

        prefix + "vis_p": _enc_vis(vis.get("place", "")),
        prefix + "vis_f": _enc_vis(vis.get("feature", "")),
        prefix + "vis_m": _enc_vis(vis.get("maneuver", "")),
        prefix + "vis_r": _enc_vis(vis.get("role", "")),

        prefix + "basis_p": _enc_basis(basis.get("place", "")),
        prefix + "basis_f": _enc_basis(basis.get("feature", "")),
        prefix + "basis_m": _enc_basis(basis.get("maneuver", "")),
        prefix + "basis_r": _enc_basis(basis.get("role", "")),

        prefix + "clear_cnt": vb_cnts["vis_clear_cnt"],
        prefix + "partial_cnt": vb_cnts["vis_partial_cnt"],
        prefix + "occ_cnt": vb_cnts["vis_occ_cnt"],
        prefix + "vis_unk_cnt": vb_cnts["vis_unknown_cnt"],

        prefix + "direct_cnt": vb_cnts["basis_direct_cnt"],
        prefix + "basis_partial_cnt": vb_cnts["basis_partial_cnt"],
        prefix + "weak_cnt": vb_cnts["basis_weak_cnt"],
        prefix + "basis_unk_cnt": vb_cnts["basis_unknown_cnt"],

        prefix + "ctr_cnt": ce_cnts["ctr_cnt"],
        prefix + "ctr_p_cnt": ce_cnts["ctr_place_cnt"],
        prefix + "ctr_f_cnt": ce_cnts["ctr_feature_cnt"],
        prefix + "ctr_m_cnt": ce_cnts["ctr_maneuver_cnt"],
        prefix + "ctr_r_cnt": ce_cnts["ctr_role_cnt"],

        prefix + "p_match": p_match, prefix + "f_match": f_match,
        prefix + "a_match": a_match, prefix + "b_match": b_match,
        prefix + "ab_match": ab_match, prefix + "exact": exact,
    }

    meta = {
        "sum16": sum16, "hard": hard, "p": p, "f": f, "m": m, "r": r,
        "clear_cnt": vb_cnts["vis_clear_cnt"], "direct_cnt": vb_cnts["basis_direct_cnt"], "weak_cnt": vb_cnts["basis_weak_cnt"],
        "exact": exact
    }
    return flat, meta

# ê²°ê³¼ íŒŒì¼ì— í•œ ì¤„ì”© ì“°ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜
def save_result_to_csv(result_dict, file_path):
    file_exists = os.path.exists(file_path)
    # ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ë¥¼ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©
    fieldnames = list(result_dict.keys())

    with open(file_path, 'a', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        # íŒŒì¼ì´ ì²˜ìŒ ìƒì„±ë˜ëŠ” ê²½ìš°ì—ë§Œ í—¤ë” ì‘ì„±
        if not file_exists:
            writer.writeheader()
        writer.writerow(result_dict)

def get_processed_videos(file_path):
    processed = set()
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path)
            if 'íŒŒì¼ëª…' in df.columns:
                processed = set(df['íŒŒì¼ëª…'].astype(str).tolist())
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")
    return processed

def get_top_10_from_csv(file_path):
    """
    CSVì˜ ì€ì„/í˜•ì„  ê°œë³„ ì„¹ì…˜ì„ íŒŒì‹±í•˜ì—¬ ë…ë¦½ì ì¸ í•©ì¹˜ ì—¬ë¶€ë¥¼ íŒë³„í•˜ê³ ,
    í†µí•© ë¶„ì„ ê²°ê³¼ ì„¹ì…˜ì—ì„œ Top-10 í›„ë³´ ë¦¬ìŠ¤íŠ¸ì™€ (P, F, A, B) í˜•ì‹ì˜ GTë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ì¸ì½”ë”© ëŒ€ì‘ (BOM ë° CP949)
    try:
        with open(file_path, 'r', encoding='utf-8-sig') as f:
            lines = f.readlines()
    except:
        with open(file_path, 'r', encoding='cp949') as f:
            lines = f.readlines()

    # 1. ê° ì„¹ì…˜ì˜ ì‹œì‘ ìœ„ì¹˜ íŒŒì•…
    eun_start, hye_start, total_start = -1, -1, -1
    for i, line in enumerate(lines):
        if "### ì€ì„_pred ê²°ê³¼ ###" in line: eun_start = i + 1
        elif "### í˜•ì„ _pred ê²°ê³¼ ###" in line: hye_start = i + 1
        elif "### í†µí•©_ë¶„ì„ ê²°ê³¼ ###" in line: total_start = i + 1

    def get_top1_scenario_id(start_idx):
        """í•´ë‹¹ ì„¹ì…˜ì˜ ìµœìƒë‹¨(1ìˆœìœ„) ì‹œë‚˜ë¦¬ì˜¤ ì¡°í•© IDë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        if start_idx == -1: return None
        section_lines = []
        for line in lines[start_idx:]:
            if line.startswith("###") or not line.strip(): break
            section_lines.append(line)

        if not section_lines: return None

        tmp_df = pd.read_csv(io.StringIO("".join(section_lines)))
        if tmp_df.empty: return None

        top = tmp_df.iloc[0]
        return f"{int(top['accident_place'])}_{int(top['accident_place_feature'])}_{int(top['vehicle_a_progress_info'])}_{int(top['vehicle_b_progress_info'])}"

    # 2. ì „ë¬¸ê°€ë³„ ë…ë¦½ì  Top-1 ì¶”ì¶œ ë° í•©ì¹˜ ì—¬ë¶€ íŒë³„
    eun_top1_id = get_top1_scenario_id(eun_start)
    hye_top1_id = get_top1_scenario_id(hye_start)
    total_top1_id = get_top1_scenario_id(total_start)
    if eun_top1_id == hye_top1_id == total_top1_id and total_top1_id is not None:
        # ì–‘ìª½ ì „ë¬¸ê°€ì™€ í†µí•© ê²°ê³¼ê°€ ëª¨ë‘ ì¼ì¹˜ (1êµ¬ê°„)
        is_agreement = "Agreement"
    elif eun_top1_id == total_top1_id and total_top1_id is not None:
        # ì€ì„ ëª¨ë¸ì˜ 1ìœ„ê°€ í†µí•© 1ìœ„ì¸ ê²½ìš° (2êµ¬ê°„)
        is_agreement = "Eunseok"
    elif hye_top1_id == total_top1_id and total_top1_id is not None:
        # í˜•ì„  ëª¨ë¸ì˜ 1ìœ„ê°€ í†µí•© 1ìœ„ì¸ ê²½ìš° (3êµ¬ê°„)
        is_agreement = "Hyeongseon"
    else:
        # ì „ë¬¸ê°€ ê°„ì˜ ì˜ê²¬ì´ ì™„ì „íˆ ê°ˆë¦¬ê±°ë‚˜ í†µí•© ê²°ê³¼ê°€ ì œ3ì˜ ì•ˆì¸ ê²½ìš° (4êµ¬ê°„)
        is_agreement = "Disagreement"

    # 3. í†µí•© ë¶„ì„ ê²°ê³¼ í…Œì´ë¸” íŒŒì‹±
    if total_start != -1:
        total_lines = [line for line in lines[total_start:] if not line.strip().startswith("###")]
        df = pd.read_csv(io.StringIO("".join(total_lines)))
        df.columns = df.columns.str.strip()
    else:
        print(f"âš ï¸ í†µí•© ë¶„ì„ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path}")
        return "Unknown", "[]", "Unknown"

    # ë°ì´í„° íƒ€ì… ì •ìˆ˜í˜• í†µì¼
    res_id_keys = ['accident_place', 'accident_place_feature', 'vehicle_a_progress_info', 'vehicle_b_progress_info']
    for k in res_id_keys:
        df[k] = pd.to_numeric(df[k], errors='coerce').fillna(-1).astype(int)

    # [ìˆ˜ì •] gt_rank ëŒ€ì‹  (P, F, A, B) í˜•ì‹ì˜ gt_str ì¶”ì¶œ
    gt_str = "Unknown"
    # lines[1]ì€ ì»¬ëŸ¼ëª…(accident_place...), lines[2]ëŠ” ì‹¤ì œ ê°’
    if len(lines) > 2 and "accident_place" in lines[1]:
        gt_header = lines[1].strip()
        gt_values = lines[2].strip()
        gt_df = pd.read_csv(io.StringIO(f"{gt_header}\n{gt_values}"))
        if not gt_df.empty:
            row = gt_df.iloc[0]
            gt_str = f"({int(row['accident_place'])}, {int(row['accident_place_feature'])}, {int(row['vehicle_a_progress_info'])}, {int(row['vehicle_b_progress_info'])})"

    # í†µí•© í…Œì´ë¸” ë‚´ì—ì„œì˜ ëª¨ë¸ë³„ ìµœëŒ“ê°’ ìœ„ì¹˜ ì°¾ê¸° (ì¶”ì²œ íƒœê·¸ìš©)
    eunseok_best_idx = df['ì€ì„_score'].idxmax() if 'ì€ì„_score' in df.columns else -1
    hyeongseon_best_idx = df['í˜•ì„ _score'].idxmax() if 'í˜•ì„ _score' in df.columns else -1

    # 4. ë§ˆìŠ¤í„° ê°€ì´ë“œ ëª…ì¹­ ê²°í•© (Top-10ë¡œ ì œí•œ)
    top_10 = df.head(10).copy()
    if 'master_df' in globals() and master_df is not None:
        top_10 = pd.merge(top_10, master_df,
                          left_on=res_id_keys,
                          right_on=['ì‚¬ê³ ì¥ì†Œ_ID', 'ì‚¬ê³ ì¥ì†ŒíŠ¹ì§•_ID', 'Aì§„í–‰ë°©í–¥_ID', 'Bì§„í–‰ë°©í–¥_ID'],
                          how='left')

    # ì¶”ì²œ íƒœê·¸ ìƒì„±
    def get_tag(idx):
        tags = []
        if idx == eunseok_best_idx: tags.append("Eunseok Top-1")
        if idx == hyeongseon_best_idx: tags.append("Hyeongseon Top-1")
        return ", ".join(tags)

    top_10['recommendation'] = [get_tag(i) for i in top_10.index]
    top_10 = top_10.fillna("")

    # VLM ì „ë‹¬ìš© ìµœì¢… ì»¬ëŸ¼ êµ¬ì„± (P, F, A, B ì½”ë“œ ì •ë³´ í¬í•¨)
    top_10['code_combination'] = top_10.apply(lambda r: f"({int(r['accident_place'])}, {int(r['accident_place_feature'])}, {int(r['vehicle_a_progress_info'])}, {int(r['vehicle_b_progress_info'])})", axis=1)

    res_df = top_10.rename(columns={
        'ìˆœìœ„': 'Rank', 'ì‚¬ê³ ì¥ì†Œ': 'place', 'ì‚¬ê³ ì¥ì†ŒíŠ¹ì§•': 'feature',
        'Aì§„í–‰ë°©í–¥': 'veh_a', 'Bì§„í–‰ë°©í–¥': 'veh_b'
    })

    final_cols = ['Rank', 'code_combination', 'place', 'feature', 'veh_a', 'veh_b', 'recommendation']

    return gt_str, res_df[final_cols], is_agreement
'''
def find_file_paths(video_stem):
    """íŒŒì¼ëª…ìœ¼ë¡œ ì‹¤ì œ ë¬¼ë¦¬ ê²½ë¡œ(Video, Label, CSV)ë¥¼ ì°¾ì•„ ë°˜í™˜"""
    for label_env, video_env in label_env_name_mapping.items():
        v_path = os.path.join(base_video_root, video_env, f"{video_stem}.mp4")
        if os.path.exists(v_path):
            l_path = os.path.join(base_label_root, label_env, f"{video_stem}.json")
            c_path = os.path.join(base_csv_root, label_env, f"{video_stem}.csv")
            return v_path, l_path, c_path
    return None, None, None
'''
def make_json(pred_str, mapping_path):
    """
    pred_str (ì˜ˆ: "(1, 11, 31, 34)")ì„ ì…ë ¥ë°›ì•„
    CSV íŒŒì¼ì˜ ID ì»¬ëŸ¼ë“¤ê³¼ ë§¤ì¹­ë˜ëŠ” í–‰ì„ ì°¾ì•„ JSON ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # 1. pred_str ë¬¸ìì—´ íŒŒì‹± (ì˜ˆ: "(1, 11, 31, 34)" -> 1, 11, 31, 34)
    try:
        # ast.literal_evalì„ ì‚¬ìš©í•˜ë©´ ê´„í˜¸ì™€ ì‰¼í‘œê°€ í¬í•¨ëœ ë¬¸ìì—´ì„ íŠœí”Œë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
        p_id, f_id, a_id, b_id = ast.literal_eval(pred_str)
    except Exception as e:
        print(f"Error parsing pred_str: {e}")
        return None

    # 2. CSV íŒŒì¼ ë¡œë“œ (ì¸ì½”ë”©ì€ ìƒí™©ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥)
    try:
        df = pd.read_csv(mapping_path, encoding='cp949')
    except UnicodeDecodeError:
        df = pd.read_csv(mapping_path, encoding='utf-8')

    # ì»¬ëŸ¼ëª… ê³µë°± ì œê±°
    df.columns = df.columns.str.strip()

    # 3. ID ì¡°ê±´ì— ë§ëŠ” í–‰ í•„í„°ë§
    condition = (
        (df['ì‚¬ê³ ì¥ì†Œ_ID'] == p_id) &
        (df['ì‚¬ê³ ì¥ì†ŒíŠ¹ì§•_ID'] == f_id) &
        (df['Aì§„í–‰ë°©í–¥_ID'] == a_id) &
        (df['Bì§„í–‰ë°©í–¥_ID'] == b_id)
    )

    match = df[condition]

    if match.empty:
        print(f"í•´ë‹¹ ì¡°í•©({pred_str})ì— ì¼ì¹˜í•˜ëŠ” ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ë§¤ì¹­ëœ ì²« ë²ˆì§¸ í–‰ ë°ì´í„° ì¶”ì¶œ
    row = match.iloc[0]

    # ê° í•­ëª©ì˜ ëª…ì¹­ (ë¬¸ìì—´ ì•ë’¤ ê³µë°± ì œê±°)
    place = str(row['ì‚¬ê³ ì¥ì†Œ']).strip()
    feature = str(row['ì‚¬ê³ ì¥ì†ŒíŠ¹ì§•']).strip()
    a_action = str(row['Aì§„í–‰ë°©í–¥']).strip()
    b_action = str(row['Bì§„í–‰ë°©í–¥']).strip()

    # 4. JSON í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
    selected_explanation_case_json = {
        "accident_type_name": f"{place}, {feature}, {a_action}, {b_action}",
        "target_code_combination": pred_str,
        "place_name": place,
        "feature_name": feature,
        "a_vehicle_action": a_action,
        "b_vehicle_action": b_action,
        "negligence_ratio_a": int(row['ê³¼ì‹¤ë¹„ìœ¨A']),
        "negligence_ratio_b": int(row['ê³¼ì‹¤ë¹„ìœ¨B'])
    }

    return selected_explanation_case_json


genai.configure(api_key=GOOGLE_API_KEY, transport='rest')
#current_model_name = "gemini-3.1-pro-preview"
current_model_name = "gemini-3-flash-preview"
model_scorer = genai.GenerativeModel(model_name=current_model_name, system_instruction=system_instruction_score_only)
model_analyzer = genai.GenerativeModel(model_name=current_model_name, system_instruction=system_instruction_explanation_direct)

#ë§¤í•‘ ì •ì˜
label_env_name_mapping = {
    "roundabout_label": "VS_ì°¨ëŒ€ì°¨_ì˜ìƒ_íšŒì „êµì°¨ë¡œ",
    "4way_signal_label": "VS_ì°¨ëŒ€ì°¨_ì˜ìƒ_ì‚¬ê±°ë¦¬êµì°¨ë¡œ(ì‹ í˜¸ë“±ìˆìŒ)",
    "road_and_other_label": "VS_ì°¨ëŒ€ì°¨_ì˜ìƒ_ì°¨ë„ì™€ì°¨ë„ê°€ì•„ë‹Œì¥ì†Œ",
    "4way_no_signal_label": "VS_ì°¨ëŒ€ì°¨_ì˜ìƒ_ì‚¬ê±°ë¦¬êµì°¨ë¡œ(ì‹ í˜¸ë“±ì—†ìŒ)",
    "highway_label": "VS_ì°¨ëŒ€ì°¨_ì˜ìƒ_ê³ ì†ë„ë¡œ(ìë™ì°¨ì „ìš©ë„ë¡œ)í¬í•¨",
    "parking_lot_label": "VS_ì°¨ëŒ€ì°¨_ì˜ìƒ_ì£¼ì°¨ì¥(ë˜ëŠ”ì°¨ë„ê°€ì•„ë‹Œì¥ì†Œ)",
    "t_junction_label": "VS_ì°¨ëŒ€ì°¨_ì˜ìƒ_Tìí˜•êµì°¨ë¡œ",
    "straight_road_label": "VS_ì°¨ëŒ€ì°¨_ì˜ìƒ_ì§ì„ ë„ë¡œ",
}

def run_explan_test(video_stem, idx, video_file, pred_str, gt_str=""):
    selected_explanation_case_json = make_json(pred_str, mapping_path)

    prompt_explanation_direct = f"""
    ì•„ë˜ëŠ” êµí†µì‚¬ê³  ë¸”ë™ë°•ìŠ¤ ì˜ìƒì— ëŒ€í•´ íŒŒì´ì¬ í›„ì²˜ë¦¬ë¡œ ì´ë¯¸ í™•ì •ëœ ì‚¬ê³ ìœ í˜• ì •ë³´ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì˜ ì—­í• ì€ ì´ í™•ì •ëœ ìœ í˜•ì„ ë°”íƒ•ìœ¼ë¡œ, ì˜ìƒì„ ì°¸ê³ í•´ ì‚¬ìš©ìì—ê²Œ ì½ê¸° ì‰¬ìš´ ì„¤ëª…ì„ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì¤‘ìš”]
    - ìµœì¢… ìœ í˜•ì€ ì´ë¯¸ í™•ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    - í›„ë³´ ë¹„êµ, ì¬ì±„ì , ì¬ì„ íƒì„ í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    - ë°˜ë“œì‹œ 'A ì°¨ëŸ‰', 'B ì°¨ëŸ‰'ìœ¼ë¡œë§Œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.
    - 'ë‚´ ì°¨ëŸ‰', 'ë¸”ë°• ì°¨ëŸ‰', 'ìƒëŒ€ ì°¨ëŸ‰', 'ê°€í•´ì°¨ëŸ‰', 'í”¼í•´ì°¨ëŸ‰' í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    - ì˜ìƒì—ì„œ ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ë‹¨ì •í•˜ì§€ ë§ê³  'í™•ì¸ ì–´ë ¤ì›€'ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

    [í™•ì •ëœ ìœ í˜• ì…ë ¥(JSON)]
    {selected_explanation_case_json}

    [ì¶œë ¥]
    {output_format_explanation_direct}
    """
    print(f"\nğŸš€ [ì„¤ëª… ì‹œì‘] {video_stem}")
    try:
        max_retries = 3
        attempt = 0
        response = None
        while attempt < max_retries:
            try:
                print("hi")
                response = model_analyzer.generate_content([prompt_explanation_direct, video_file])
                print("bye")
                break # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ

            except (http.client.RemoteDisconnected, Exception) as e:
                if ("429" in str(e) or "Quota" in str(e)):
                    print(f"ğŸš¨ í• ë‹¹ëŸ‰ ì´ˆê³¼! {current_model_name}ì˜ í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                    continue

                attempt += 1
                print(f"âš ï¸ {attempt}ì°¨ ì‹œë„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                time.sleep(10)

                if attempt >= max_retries:
                    print(f"âŒ ìµœì¢… ì‹¤íŒ¨: {video_stem}")
                    raise e

        vlm_text = response.text

        # 1. ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±° ë° JSON íŒŒì‹± (LLMì´ ```json ... ``` í˜•íƒœë¡œ ì¶œë ¥í•  ê²½ìš° ëŒ€ë¹„)
        clean_json_str = re.sub(r"```json\s*", "", vlm_text)
        clean_json_str = re.sub(r"```\s*$", "", clean_json_str).strip()

        vlm_json = json.loads(clean_json_str)

        # [ì¶”ê°€ëœ ë¡œì§] ë§Œì•½ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ë©´ ì²« ë²ˆì§¸ ìš”ì†Œ(ë”•ì…”ë„ˆë¦¬)ë¥¼ ì„ íƒ
        if isinstance(vlm_json, list):
            if len(vlm_json) > 0:
                vlm_json = vlm_json[0]
            else:
                raise ValueError("Empty JSON list received")

        # ì¤‘ì²©ëœ JSON êµ¬ì¡°ì—ì„œ í•„ìš”í•œ ê°’ë“¤ì„ ì¶”ì¶œí•˜ì—¬ í‰íƒ„í™”(Flatten)í•©ë‹ˆë‹¤.
        obs = vlm_json.get("video_observation", {})
        scene = obs.get("scene_condition", {})
        road = obs.get("road_context", {})
        movement = obs.get("movement_observation", {})

        # ë¶ˆí™•ì‹¤ì„± ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        uncertainties = ", ".join(vlm_json.get("uncertainties", []))

        # 2. ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— append (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
        # selected_explanation_case_jsonì— ìˆëŠ” ì •ë³´ì™€ VLMì´ ìƒì„±í•œ ìƒì„¸ ë¶„ì„ì„ í†µí•©í•©ë‹ˆë‹¤.
        results_exp_list=[]
        results_exp_list.append({
            "video_stem": video_stem,  # íŒŒì¼ëª… ë“± ì‹ë³„ì

            # Mapping ë°ì´í„° (ì´ì „ ë‹¨ê³„ì—ì„œ ë§Œë“  json ë°ì´í„° í™œìš©)
            #"gt_code_combination": gt_str,
            "target_code_combination": selected_explanation_case_json.get("target_code_combination"),
            "accident_type_name": selected_explanation_case_json.get("accident_type_name"),
            "negligence_ratio_a": selected_explanation_case_json.get("negligence_ratio_a"),
            "negligence_ratio_b": selected_explanation_case_json.get("negligence_ratio_b"),

            # VLM ìƒì„¸ ê´€ì°° ë°ì´í„° (JSON íŒŒì‹± ê²°ê³¼)
            "time_of_day": scene.get("time_of_day"),
            "weather": scene.get("weather"),
            "visibility_note": scene.get("visibility_note"),
            "intersection_type": road.get("intersection_type_observed"),
            "signal_observed": road.get("signal_observed"),
            "road_scale_hint": road.get("road_scale_hint"),
            "a_vehicle_observation": movement.get("a_vehicle_observation"),
            "b_vehicle_observation": movement.get("b_vehicle_observation"),
            "collision_moment": movement.get("collision_moment"),
            "uncertainties": uncertainties,

            # ìµœì¢… ì„¤ëª… ë¬¸êµ¬
            "explanation_text": vlm_json.get("explanation_text")
        })
    except Exception as e:
        print(str(e))
        print("Why")
        return False
    return vlm_json.get("explanation_text")

def run_score_test(video_stem, idx, video_file, c_path):
    #global model_scorer, current_model_name

    # ë°ì´í„° ì¤€ë¹„
    gt_str, candidates_df, is_agreement = get_top_10_from_csv(c_path)
    gt_str = ""
    if is_agreement == "Agreement":
        return False, candidates_df['code_combination'].iloc[0], gt_str
        pruned_df = candidates_df[candidates_df['Rank'].isin([1, 2, 3])]
    elif is_agreement == "Eunseok":
        pruned_df = candidates_df[candidates_df['recommendation'].str.contains("Top-1", na=False)]
    elif is_agreement == "Hyeongseon":
        #current_section_instruction = common_instruction + "\n" + section_3_instruction_6_2
        pruned_df = candidates_df[candidates_df['recommendation'].str.contains("Top-1", na=False)]
    elif is_agreement == "Disagreement":
        pruned_df = candidates_df[(candidates_df['recommendation'].str.contains("Top-1", na=False)) | (candidates_df['Rank'] == 1)]
        # ë™ì¼í•œ ì½”ë“œê°€ ì¤‘ë³µ ì„ íƒë˜ëŠ” ê²ƒì„ ë°©ì§€
        pruned_df = pruned_df.drop_duplicates(subset=['code_combination'])

    print(f"\nğŸš€ [ë¶„ì„ ì‹œì‘] {video_stem}")

    # --- [ì…ë ¥ JSON ê°•í™” ë¡œì§ ì¶”ê°€] ---
    pruned_df = pruned_df.reset_index(drop=True)

    # 1. hypothesis_id ë¶€ì—¬ (H1, H2, H3)
    pruned_df['hypothesis_id'] = [f"H{i+1}" for i in range(len(pruned_df))]


    def extract_code_tuple(s):
        nums = re.findall(r'\d+', str(s))
        return tuple(map(int, nums[:4])) if len(nums) >= 4 else None

    #gt_tuple = extract_code_tuple(gt_str)
    #is_gt_in_candidates = any(extract_code_tuple(c) == gt_tuple for c in pruned_df['code_combination'])

    # 2. source_tag ìƒì„± (ì¶”ì²œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ)
    def get_source_tag(row):
        rec = str(row.get('recommendation', ''))
        rank = row.get('Rank', None)

        if is_agreement == "Agreement":
            if rank == 1:
                return "Agreement_Rank_1"
            elif rank == 2:
                return "Agreement_Rank_2"
            elif rank == 3:
                return "Agreement_Rank_3"
            return "Agreement_Rank_3"
        if "Eunseok" in rec: return "Eunseok"
        if "Hyeongseon" in rec: return "Hyeongseon"
        return "Integrated" # Rank 1 ë“±
    pruned_df['source_tag'] = pruned_df.apply(get_source_tag, axis=1)

    # 3. í•„ë“œëª… ë§¤í•‘ ë° ê·œê²©í™” (target ìƒì„±)
    pruned_df['target_code_combination'] = pruned_df['code_combination']
    pruned_df['target'] = pruned_df.apply(
        lambda r: f"{r['code_combination']}: ({r['place']}, {r['feature']}, {r['veh_a']}, {r['veh_b']})", axis=1
    )

    # 4. VLMì— ì „ë‹¬í•  ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•˜ì—¬ JSON ë³€í™˜
    vlm_input_cols = ['hypothesis_id', 'source_tag', 'target_code_combination', 'target', 'place', 'feature', 'veh_a', 'veh_b']
    selected_candidates_json = pruned_df[vlm_input_cols].to_json(orient='records', force_ascii=False)

    # 5. í›„ì²˜ë¦¬ë¥¼ ìœ„í•œ ID ë§¤í•‘ í…Œì´ë¸” ë¯¸ë¦¬ ìƒì„±
    id_to_numeric_map = dict(zip(pruned_df['hypothesis_id'], pruned_df['target_code_combination']))

    prompt_score_only = f"""
    ì•„ë˜ëŠ” êµí†µì‚¬ê³  ë¸”ë™ë°•ìŠ¤ ì˜ìƒì— ëŒ€í•œ í›„ë³´ ê°€ì„¤ ëª©ë¡ì…ë‹ˆë‹¤.
    ì´ë²ˆ ì‘ì—…ì€ ìµœì¢… ì„ íƒì´ ì•„ë‹ˆë¼, ê° í›„ë³´ì˜ ì‹œê°ì  ì¼ì¹˜ë„ ì±„ì (score-only)ì…ë‹ˆë‹¤.

    [ì…ë ¥ê°’ ìœ ì§€ ê·œì¹™]
    - hypothesis_id, target_code_combination, target, source_tagëŠ” ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì‹­ì‹œì˜¤. ì„ì˜ ìˆ˜ì • ê¸ˆì§€.
    - hypothesis_scoringì—ëŠ” ì…ë ¥ëœ ëª¨ë“  í›„ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ì‹­ì‹œì˜¤.

    [ì‹¤í–‰ ì§€ì‹œ]
    - ëª¨ë“  í›„ë³´ë¥¼ ê°™ì€ ê¸°ì¤€ìœ¼ë¡œ ì±„ì í•˜ì‹­ì‹œì˜¤.
    - ê° í›„ë³´ë§ˆë‹¤ counter_evidenceë¥¼ ìµœì†Œ 1ê°œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    (ë°˜ì¦ì´ ì—†ìœ¼ë©´ {{"time":"None","type":"None","detail":"None"}} ì‚¬ìš©)
    - evidence / environment_cues / counter_evidence / axis_comparison.notes ë°°ì—´ì€ ê°ê° ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

    [ì¶”ê°€ ê´€ì°° ê¸°ì¤€: Place ê³„ì¸µ íŒë‹¨]
    - ì•„ë˜ ì§€ì¹¨ì€ place/feature ê´€ì°°ì„ ì •ë¦¬í•˜ê¸° ìœ„í•œ ì°¸ê³  ê¸°ì¤€ì…ë‹ˆë‹¤.
    - hard-ruleë¡œ ê°•ì œí•˜ì§€ ë§ê³ , ì˜ìƒì—ì„œ ì‹¤ì œë¡œ ë³´ì´ëŠ” ë‹¨ì„œë¥¼ ìš°ì„ í•˜ì‹­ì‹œì˜¤.
    {place_hierarchy_instruction}

    [í›„ë³´ ê°€ì„¤(JSON)]
    {selected_candidates_json}

    [ì¶œë ¥]
    {output_format_score_only}
    """

    if selected_candidates_json=="[]" or selected_candidates_json == "{}":
        print("csv íŒŒì‹± ì˜¤ë¥˜")
        return False, "(-1,-1,-1,-1)", gt_str
    try:
        max_retries = 3
        attempt = 0
        response = None
        while attempt < max_retries:
            try:
                response = model_scorer.generate_content([prompt_score_only, video_file])
                break # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ

            except (http.client.RemoteDisconnected, Exception) as e:
                if ("429" in str(e) or "Quota" in str(e)):
                    print(f"ğŸš¨ í• ë‹¹ëŸ‰ ì´ˆê³¼! {current_model_name}ì˜ í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                    continue

                attempt += 1
                print(f"âš ï¸ {attempt}ì°¨ ì‹œë„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                time.sleep(10)

                if attempt >= max_retries:
                    print(f"âŒ ìµœì¢… ì‹¤íŒ¨: {video_stem}")
                    raise e

        vlm_text = response.text

        # 1. ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±° ë° JSON íŒŒì‹± (LLMì´ ```json ... ``` í˜•íƒœë¡œ ì¶œë ¥í•  ê²½ìš° ëŒ€ë¹„)
        clean_json_str = re.sub(r"```json\s*", "", vlm_text)
        clean_json_str = re.sub(r"```\s*$", "", clean_json_str).strip()

        vlm_json = json.loads(clean_json_str)

        # [ì¶”ê°€ëœ ë¡œì§] ë§Œì•½ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ë©´ ì²« ë²ˆì§¸ ìš”ì†Œ(ë”•ì…”ë„ˆë¦¬)ë¥¼ ì„ íƒ
        if isinstance(vlm_json, list):
            if len(vlm_json) > 0:
                vlm_json = vlm_json[0]
            else:
                raise ValueError("Empty JSON list received")

        # score-onlyì—ì„œëŠ” hypothesis_scoringë§Œ ì‚¬ìš©
        h_scores = vlm_json.get("hypothesis_scoring", [])
        if not isinstance(h_scores, list):
            h_scores = []

        # hypothesis_id ê¸°ì¤€ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì°¾ê¸° ìœ„í•œ dict
        h_score_map = {}
        for h in h_scores:
            h_id = str(h.get("hypothesis_id", "")).strip()
            if h_id:
                h_score_map[h_id] = h

        # í›„ë³´/GT íŒŒì‹±ìš© í•¨ìˆ˜
        def parse_code(code_str):
            nums = re.findall(r'\d+', str(code_str))
            return [int(n) for n in nums] if len(nums) >= 4 else [None, None, None, None]

        # GT íŒŒì‹±
        #g_p, g_f, g_a, g_b = parse_code(gt_str)

        # score-only ë‹¨ê³„ì—ì„œëŠ” ìµœì¢… ì„ íƒ ì—†ìŒ
        # ëŒ€ì‹  í›„ë³´ë³„ ì ìˆ˜ë§Œ ì €ì¥í•˜ê³ , ë‚˜ì¤‘ì— íŒŒì´ì¬ì—ì„œ í›„ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ì¶©ë¶„íˆ ê¸°ë¡
        h_data = {}

        # pruned_df ê¸°ì¤€ìœ¼ë¡œ ê¸°ëŒ€ í›„ë³´(H1/H2/H3) ìˆœì„œëŒ€ë¡œ ì €ì¥ (VLM ìˆœì„œ ê¼¬ì—¬ë„ ì•ˆì „)
        for i in range(1, 4):
            if len(pruned_df) >= i:
                row = pruned_df.iloc[i-1]
                expected_hid = str(row.get("hypothesis_id", f"H{i}"))
                expected_code = str(row.get("target_code_combination", row.get("code_combination", "")))
                expected_target = str(row.get("target", ""))
                expected_source = str(row.get("source_tag", ""))

                h = h_score_map.get(expected_hid, {})

                s = h.get("scores", {}) if isinstance(h.get("scores", {}), dict) else {}
                sr = h.get("score_reasons", {}) if isinstance(h.get("score_reasons", {}), dict) else {}

                vis = h.get("visibility", {}) if isinstance(h.get("visibility", {}), dict) else {}
                basis = h.get("basis", {}) if isinstance(h.get("basis", {}), dict) else {}

                contr_axes = h.get("contradiction_axes", [])
                if not isinstance(contr_axes, list):
                    contr_axes = []
                contr_axes_str = ",".join([str(x) for x in contr_axes])

                def _safe_score(v):
                    try:
                        return int(float(v))
                    except:
                        return -1

                p_score = _safe_score(s.get("place_score", -1))
                f_score = _safe_score(s.get("feature_score", -1))
                m_score = _safe_score(s.get("maneuver_score", -1))
                r_score = _safe_score(s.get("role_score", -1))

                # ë°˜ì¦ ë¬¸ìì—´ ì§ë ¬í™”
                ce_list = h.get("counter_evidence", [])
                if not isinstance(ce_list, list):
                    ce_list = []
                ce_str = " | ".join([
                    f"[{ce.get('time','')}/{ce.get('type','')}] {ce.get('detail','')}"
                    for ce in ce_list if isinstance(ce, dict)
                ]) if ce_list else "[None/None] None"

                # score-only ë‹¨ê³„ìš© í•©ê³„ (í›„ì²˜ë¦¬ ì „ ì„ì‹œ ë¶„ì„ìš©)
                raw_sum = sum([x for x in [p_score, f_score, m_score, r_score] if isinstance(x, int) and x >= 0])

                h_data[f"ê°€ì„¤{i}_ID"] = expected_hid
                h_data[f"ê°€ì„¤{i}_ì…ë ¥ì½”ë“œ"] = expected_code
                h_data[f"ê°€ì„¤{i}_ì…ë ¥íƒ€ê²Ÿ"] = expected_target
                h_data[f"ê°€ì„¤{i}_ì¶œì²˜"] = expected_source

                # VLMì´ targetì„ ê·¸ëŒ€ë¡œ ì•ˆ ëŒë ¤ì¤˜ë„ ì…ë ¥ ê¸°ì¤€ìœ¼ë¡œ ë³´ì¡´
                h_data[f"ê°€ì„¤{i}_VLMíƒ€ê²Ÿ"] = h.get("target", "")
                h_data[f"ê°€ì„¤{i}_í•˜ë“œëª¨ìˆœ"] = h.get("hard_contradiction", False)

                h_data[f"ê°€ì„¤{i}_ì ìˆ˜_P"] = p_score
                h_data[f"ê°€ì„¤{i}_ì ìˆ˜_F"] = f_score
                h_data[f"ê°€ì„¤{i}_ì ìˆ˜_M"] = m_score
                h_data[f"ê°€ì„¤{i}_ì ìˆ˜_R"] = r_score
                h_data[f"ê°€ì„¤{i}_ì ìˆ˜í•©(0~16)"] = raw_sum

                h_data[f"ê°€ì„¤{i}_ê·¼ê±°_P"] = sr.get("place_reason", "")
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°_F"] = sr.get("feature_reason", "")
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°_M"] = sr.get("maneuver_reason", "")
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°_R"] = sr.get("role_reason", "")

                h_data[f"ê°€ì„¤{i}_ë°˜ì¦"] = ce_str
                h_data[f"ê°€ì„¤{i}_ëª¨ìˆœì¶•"] = contr_axes_str

                h_data[f"ê°€ì„¤{i}_ê°€ì‹œì„±_P"] = vis.get("place", "")
                h_data[f"ê°€ì„¤{i}_ê°€ì‹œì„±_F"] = vis.get("feature", "")
                h_data[f"ê°€ì„¤{i}_ê°€ì‹œì„±_M"] = vis.get("maneuver", "")
                h_data[f"ê°€ì„¤{i}_ê°€ì‹œì„±_R"] = vis.get("role", "")

                h_data[f"ê°€ì„¤{i}_ê·¼ê±°ê°•ë„_P"] = basis.get("place", "")
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°ê°•ë„_F"] = basis.get("feature", "")
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°ê°•ë„_M"] = basis.get("maneuver", "")
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°ê°•ë„_R"] = basis.get("role", "")
            else:
                # í›„ë³´ê°€ 2ê°œì¸ êµ¬ê°„ ëŒ€ë¹„ ë¹ˆì¹¸ ì±„ìš°ê¸°
                h_data[f"ê°€ì„¤{i}_ID"] = ""
                h_data[f"ê°€ì„¤{i}_ì…ë ¥ì½”ë“œ"] = ""
                h_data[f"ê°€ì„¤{i}_ì…ë ¥íƒ€ê²Ÿ"] = ""
                h_data[f"ê°€ì„¤{i}_ì¶œì²˜"] = ""
                h_data[f"ê°€ì„¤{i}_VLMíƒ€ê²Ÿ"] = ""
                h_data[f"ê°€ì„¤{i}_í•˜ë“œëª¨ìˆœ"] = ""
                h_data[f"ê°€ì„¤{i}_ì ìˆ˜_P"] = ""
                h_data[f"ê°€ì„¤{i}_ì ìˆ˜_F"] = ""
                h_data[f"ê°€ì„¤{i}_ì ìˆ˜_M"] = ""
                h_data[f"ê°€ì„¤{i}_ì ìˆ˜_R"] = ""
                h_data[f"ê°€ì„¤{i}_ì ìˆ˜í•©(0~16)"] = ""
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°_P"] = ""
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°_F"] = ""
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°_M"] = ""
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°_R"] = ""
                h_data[f"ê°€ì„¤{i}_ë°˜ì¦"] = ""
                h_data[f"ê°€ì„¤{i}_ëª¨ìˆœì¶•"] = ""
                h_data[f"ê°€ì„¤{i}_ê°€ì‹œì„±_P"] = ""
                h_data[f"ê°€ì„¤{i}_ê°€ì‹œì„±_F"] = ""
                h_data[f"ê°€ì„¤{i}_ê°€ì‹œì„±_M"] = ""
                h_data[f"ê°€ì„¤{i}_ê°€ì‹œì„±_R"] = ""
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°ê°•ë„_P"] = ""
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°ê°•ë„_F"] = ""
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°ê°•ë„_M"] = ""
                h_data[f"ê°€ì„¤{i}_ê·¼ê±°ê°•ë„_R"] = ""

        # score-onlyì—ì„œë„ ê´€ì°° í•„ë“œëŠ” ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ìˆìœ¼ë©´ ì €ì¥ (ì—†ìœ¼ë©´ ë¹ˆê°’)
        visual_obs = vlm_json.get("visual_observation", {}) if isinstance(vlm_json.get("visual_observation", {}), dict) else {}
        role_id = vlm_json.get("role_identification", {}) if isinstance(vlm_json.get("role_identification", {}), dict) else {}
        pov_obs = vlm_json.get("pov_observation", {}) if isinstance(vlm_json.get("pov_observation", {}), dict) else {}

        ego_is = role_id.get("blackbox_is", "unknown")
        #gt_other = gt_ab_dict.get(video_stem, "Unknown").upper()
        #gt_ego_true = 'B' if 'A' in gt_other else ('A' if 'B' in gt_other else 'Unknown')
        ego_is_clean = 'A' if 'A' in str(ego_is).upper() else ('B' if 'B' in str(ego_is).upper() else 'Unknown')
        #is_ego_correct = "Pass" if (gt_ego_true == ego_is_clean and gt_ego_true != "Unknown") else "Fail"

        # ìµœì¢… ì„ íƒ/ì •ë‹µ ì ì¤‘ì€ ì•„ì§ ê³„ì‚° ì•ˆí•¨ (í›„ì²˜ë¦¬ ì „)
        results_list= []
        results_list.append({
            "íŒŒì¼ëª…": video_stem,
            "êµ¬ê°„ìœ í˜•": is_agreement,
            #"GT_ì½”ë“œ": gt_str,

            # score-only ë‹¨ê³„ ìƒíƒœ
            "í›„ë³´ê°œìˆ˜": len(pruned_df),
            #"í›„ë³´ë‚´ì •ë‹µì¡´ì¬": "Yes" if is_gt_in_candidates else "No",

            # ê´€ì°°/ë³´ì¡° ì •ë³´
            "ì¹´ë©”ë¼ì‹œì ì¶”ì •": pov_obs.get("camera_view", ""),
            "ë„ë¡œí† í´ë¡œì§€ì¶”ì •": visual_obs.get("road_topology_guess", ""),
            "ê¸°ë™ê´€ì°°_Ego": visual_obs.get("ego_maneuver_guess", ""),
            "ê¸°ë™ê´€ì°°_Other": visual_obs.get("other_vehicle_maneuver_guess", ""),
            "ì¶©ëŒê¸°í•˜": visual_obs.get("collision_geometry", ""),
            #"GT_ë¸”ë°•ì°¨ëŸ‰": gt_ego_true,
            "VLM_ë¸”ë°•ì°¨ëŸ‰": ego_is_clean,
            #"ë¸”ë°•ì‹ë³„_ì„±ê³µì—¬ë¶€": is_ego_correct,

            # ì¶”ê°€ í•„ë“œ
            "ì¹´ë©”ë¼ì‹œì ì¶”ì •_ì‹ ë¢°ë„": pov_obs.get("confidence", ""),
            "ê´€ì°°ì‹ ë¢°ë„": visual_obs.get("observation_confidence", ""),
            "ì—­í• ë§¤í•‘ê·¼ê±°": role_id.get("mapping_reason", ""),
            "ì—­í• ì‹ë³„ì‹ ë¢°ë„": role_id.get("confidence", ""),

            # ë””ë²„ê¹…ìš© ì›ë¬¸/íŒŒì‹± ìƒíƒœ
            "íŒŒì‹±ìƒíƒœ": "OK",
            "ëª¨ë¸": current_model_name,

            # ê°€ì„¤ë³„ ìƒì„¸
            **h_data
        })

        # --------------------------------------------------------
        # build compact candidates (H1/H2/H3)
        # --------------------------------------------------------
        h1_flat, h1m = _pack_h(1, pruned_df, h_score_map, g_p, g_f, g_a, g_b)
        h2_flat, h2m = _pack_h(2, pruned_df, h_score_map, g_p, g_f, g_a, g_b)
        h3_flat, h3m = _pack_h(3, pruned_df, h_score_map, g_p, g_f, g_a, g_b)

        # rank by sum (tie: hard ì‘ì€ í›„ë³´ ìš°ì„ )
        _hrows_rank = {1: h1m, 2: h2m, 3: h3m}
        top1_idx, top2_idx, top1_sum, top2_sum = _argmax_hid_by_sum(_hrows_rank, int(len(pruned_df)))

        # small helper for delta
        def _delta(a, b, invalid=-99):
            return (a - b) if (a is not None and b is not None and a >= 0 and b >= 0) else invalid

        # --------------------------------------------------------
        # short analysis row append (main block stays compact)
        # --------------------------------------------------------
        results_short_list = []
        results_short_list.append({
            # meta
            "schema_ver": 2,
            "row_id": f"{video_stem}__{_enc_section(is_agreement)}",
            "video_id": video_stem,
            "section_code": _enc_section(is_agreement),   # 1/2/3/4/0
            "n_cands": int(len(pruned_df)),
            #"gt_in_cands": 1 if is_gt_in_candidates else 0,
            "parse_ok": 1,

            # GT split
            #"gt_p": g_p if g_p is not None else -1,
            #"gt_f": g_f if g_f is not None else -1,
            #"gt_a": g_a if g_a is not None else -1,
            #"gt_b": g_b if g_b is not None else -1,

            # observation / role (encoded)
            "cam_view": _enc_cam(pov_obs.get("camera_view", "ë¶ˆëª…")),
            "cam_conf": _enc_conf(pov_obs.get("confidence", "")),
            "road_topo": _enc_road(visual_obs.get("road_topology_guess", "ë¶ˆëª…")),
            "obs_conf": _enc_conf(visual_obs.get("observation_confidence", "")),
            "role_conf": _enc_conf(role_id.get("confidence", "")),
            #"ego_gt": _enc_ab(gt_ego_true),
            "ego_pred": _enc_ab(ego_is_clean),
            #"ego_pass": 1 if str(is_ego_correct).lower() == "pass" else 0,

            # flattened candidates
            **h1_flat, **h2_flat, **h3_flat,

            # baseline (H1)
            "base_idx": 1,
            "base_exact": h1m["exact"],

            # rank summary (pure score sum)
            "top1_idx": top1_idx,
            "top2_idx": top2_idx,
            "top1_sum16": top1_sum,
            "top2_sum16": top2_sum,
            "top12_margin_sum": (top1_sum - top2_sum) if (top1_sum >= 0 and top2_sum >= 0) else -99,
            "top1_exact": (h1m["exact"] if top1_idx == 1 else h2m["exact"] if top1_idx == 2 else h3m["exact"] if top1_idx == 3 else 0),
            "top2_exact": (h1m["exact"] if top2_idx == 1 else h2m["exact"] if top2_idx == 2 else h3m["exact"] if top2_idx == 3 else 0),

            # commonly used H2 vs H1 deltas (threshold searchìš©)
            "h2m1_p": _delta(h2m["p"], h1m["p"]),
            "h2m1_f": _delta(h2m["f"], h1m["f"]),
            "h2m1_m": _delta(h2m["m"], h1m["m"]),
            "h2m1_r": _delta(h2m["r"], h1m["r"]),
            "h2m1_sum16": _delta(h2m["sum16"], h1m["sum16"]),
            "h2m1_clear_cnt": h2m["clear_cnt"] - h1m["clear_cnt"],
            "h2m1_direct_cnt": h2m["direct_cnt"] - h1m["direct_cnt"],
            "h2m1_weak_cnt": h2m["weak_cnt"] - h1m["weak_cnt"],

            # optional: H3 vs H1 / H3 vs H2ë„ ìì£¼ ë³´ë©´ ì¶”ê°€
            "h3m1_sum16": _delta(h3m["sum16"], h1m["sum16"]),
            "h3m2_sum16": _delta(h3m["sum16"], h2m["sum16"]),

            "model": current_model_name,
        })

        #print(f"{idx}: [{video_stem}] ì²˜ë¦¬ ì™„ë£Œ - GT: {gt_str}")#/ VLM: {vlm_codes_str} ({exact_match})")

    except KeyboardInterrupt:
        print(f"\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨: {video_stem}")
        raise  # ë£¨í”„ ì „ì²´ë¥¼ ë©ˆì¶”ë ¤ë©´ ë‹¤ì‹œ raise

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ({video_stem}): {e}")
        return False, "(-1,-1,-1,-1)", gt_str

    final_pred_code = h_data.get(f"ê°€ì„¤{top1_idx}_ì…ë ¥ì½”ë“œ", "(-1,-1,-1,-1)")
    return True, final_pred_code, gt_str



'''
sec1_samples = ['bb_1_160910_vehicle_241_26225', 'bb_1_160116_vehicle_222_29116', 'bb_1_180222_vehicle_148_238', 'bb_1_160614_vehicle_112_113', 'bb_1_120318_vehicle_113_157', 'bb_1_220827_vehicle_256_50486', 'bb_1_170120_vehicle_233_22160', 'bb_1_150517_vehicle_37_150', 'bb_1_210210_vehicle_212_45839', 'bb_1_181104_vehicle_195_096']

target_samples = sec1_samples

output_csv_path = "/content/drive/MyDrive/260224_ai/4th_experiment_results_score.csv"
output_short_csv_path = "/content/drive/MyDrive/260224_ai/4th_experiment_results_score_short.csv"
output_exp_csv_path = "/content/drive/MyDrive/260224_ai/4th_experiment_results_exp.csv"
processed_in_this_env = get_processed_videos(output_exp_csv_path)
video_files = [v for v in target_samples if v not in processed_in_this_env] # ë¯¸ì²˜ë¦¬ íŒŒì¼ë§Œ ì¶”ì¶œ


input_c_path = "/content/input.csv"

print(f"{len(processed_in_this_env)}, {len(video_files)}")

for i, sample in enumerate(video_files):
    try:
        v_path, l_path, _ = find_file_paths(sample)

        process_single_json_to_csv(l_path, input_c_path, csv_type_path)

        if not input_c_path:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {sample}")
        else:
            video_file = genai.upload_file(path=v_path)
            while video_file.state.name == "PROCESSING":
                time.sleep(2)
                video_file = genai.get_file(video_file.name)

        run, pred_str, gt_str = run_score_test(sample, i, video_file, input_c_path)

        if results_list and False:
            last_result = results_list[-1]
            save_result_to_csv(last_result, output_csv_path)
            last_result_short = results_short_list[-1]
            save_result_to_csv(last_result_short, output_short_csv_path)

        # API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        time.sleep(3)

        #pred_str = "(-1,-1,-1,-1)"
        run2 = run_explan_test(sample, i, video_file, pred_str, gt_str)

        if results_exp_list and run2:
            last_exp_result = results_exp_list[-1]
            save_result_to_csv(last_exp_result, output_exp_csv_path)

        # API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        
    except Exception as e:
        print(f"âŒ {sample} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ(ê±´ë„ˆëœ€): {e}")
        continue

    finally:
        if 'video_file' in locals():
            try:
                video_file.delete()
            except Exception as e:
                print("ë¹„ë””ì˜¤ ì‚­ì œ ì‹¤íŒ¨")
'''

